{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57a2669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nनादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है \\n गलियों में खून की नदियाँ बह रही हैं \\n चारों तरफ हाहाकार मचा हुआ है \\n बाजार बंद हैं \\n दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं \\n किसी की जान सलामत नहीं है \\n कहीं घरों में आग लगी हुई है  कहीं बाजार लुट रहा है  कोई किसी की फरियाद नहीं सुनता \\n रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है \\n ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती \\n मानव हृदय की क्रूरता  कठोरता और पैशाचिकता अपना विकरालतम रूप धारण किये हुए हैं \\n इसी समय नादिरशाह ने बादशाही महल में प्रवेश किया\\nदिल्ली उन दिनों भोगविलास की केंद्र बनी हुई थी \\n सजावट और तकल्लुफ के सामानों से रईसों के भवन भरे रहते थे \\n स्त्रियों को बनाव सिंगार के सिवा कोई काम न था \\n पुरुषों को सुख भोग के सिवा और कोई चिंता न थी \\n राजनीति का स्थान शेरो शायरी ने ले लिया था \\n समस्त प्रान्तों से धन खिंच खिंचकर दिल्ली आता था और पानी की भाँति बहाया जाता था \\n वेश्याओं की चाँदी थी \\n कहीं तीतरों के जोड़ होते थे  कहीं बटेरों और बुलबुलों की पालियाँ ठनती थीं \\n सारा नगर विलास निद्रा में मग्न था \\n नादिरशाह शाही महल में पहुँचा तो वहाँ का सामान देखकर उसकी आँखें खुल गयीं \\n उसका जन्म दरिद्र घर में हुआ था \\n उसका समस्त जीवन रणभूमि में ही कटा था \\n भोगविलास का उसे चसका न लगा था \\n कहाँ रणक्षेत्र के कष्ट और कहाँ यह सुख साम्राज्य \\n जिधर आँख उठती थी  उधर से हटने का नाम न लेती थी\\nसंध्या हो गयी थी \\n नादिरशाह अपने सरदारों के साथ महल की सैर करता और अपनी पसंद की चीजों को बटोरता हुआ दीवाने खास में आकर कारचोबी मसनद पर बैठ गया  सरदारों को वहाँ से चले जाने का हुक्म दे दिया  अपने सब हथियार खोलकर रख दिये और महल में द़रोगा को बुलाकर हुक्म दिया  मैं शाही बेगमों का नाच देखना चाहता हूँ \\n तुम इसी वक्त उनको सुंदर वस्त्रभूषणों से सजाकर मेरे सामने लाओ \\n खबरदार  जरा भी देर न हो \\n मैं कोई उज्र या इनकार नहीं सुन सकता\\nदारोगा ने यह नादिरशाही हुक्म सुना तो होश उड़ गये \\n वह महिलाएँ जिन पर कभी सूर्य की दृष्टि भी नहीं पड़ी कैसे इस मजलिस में आयेंगी \\n नाचने का तो कहना ही क्या \\n शाही बेगमों का इतना अपमान कभी न हुआ था \\n हा नरपिशाच \\n दिल्ली को खून से रँगकर भी तेरा चित्त शांत नहीं हुआ \\n मगर नादिरशाह के सम्मुख एक शब्द भी जबान से निकालना अग्नि के मुख में कूदना था \\n सिर झुकाकर आदाब बजा लाया और आकर रनिवास में सब बेगमों को नादिरशाही हुक्म सुना दिया  उसके साथ ही यह इत्तला भी दे दी कि जरा भी ताम्मुल न हो  नादिरशाह कोई उज्र या हीला न सुनेगा \\n शाही खानदान पर इतनी बड़ी विपत्ति कभी नहीं पड़ी  पर इस समय विजयी बादशाह की आज्ञा को शिरोधार्य करने के सिवा प्राण रक्षा का अन्य कोई उपाय नहीं था\\nबेगमों ने यह आज्ञा सुनी तो हतबुद्धि सी हो गयीं \\n सारे रनिवास में मातम सा छा गया \\n वह चहल पहल गायब हो गयी \\n सैकड़ों हृदयों से इस अत्याचारी के प्रति एक शाप निकल गया \\n किसी ने आकाश की ओर सहायतायाचक लोचनों से देखा  किसी ने खुदा और रसूल को सुमिरन किया  पर ऐसी एक महिला भी न थी जिसकी निगाह कटार या तलवार की तरफ गयी हो \\n यद्यपि इनमें कितनी ही बेगमों के नसों में राजपूतानियों का रक्त प्रवाहित हो रहा था  पर इंद्रियलिप्सा ने जौहर की पुरानी आग ठंडी कर दी थी \\n सुखभोग की लालसा आत्म सम्मान का सर्वनाश कर देती है \\n आपस में सलाह करके मर्यादा की रक्षा का कोई उपाय सोचने की मुहलत न थी \\n एक एक पल भाग्य का निर्णय कर रहा था \\n हताश होकर सभी ललनाओं ने पापी के सम्मुख जाने का निश्चय किया \\n आँखों से आँसू जारी थे  दिलों से आहें निकल रही थीं  पर रत्न जटित आभूषण पहने जा रहे थे  अश्रु सिंचित नेत्रों में सुरमा लगाया जा रहा था और शोक व्यथित हृदयों पर सुगंध का लेप किया जा रहा था \\n कोई केश गूँथती थी  कोई माँगों में मोतियाँ पिरोती थी \\n एक भी ऐसे पक्के इरादे की स्त्री न थी  जो ईश्वर पर अथवा अपनी टेक पर  इस आज्ञा का उल्लंघन करने का साहस कर सके\\nएक घंटा भी न गुजरने पाया था कि बेगमात पूरे के पूरे आभूषणों से जगमगाती  अपने मुख की कांति से बेले और गुलाब की कलियों को लजाती  सुगंध की लपटें उड़ाती  छमछम करती हुई दीवाने खास में आकर नादिरशाह के सामने खड़ी हो गयीं\\nनादिरशाह ने एक बार कनखियों से परियों के इस दल को देखा और तब मसनद की टेक लगाकर लेट गया \\n अपनी तलवार और कटार सामने रख दीं \\n एक क्षण में उसकी आँखें झपकने लगीं \\n उसने एक अँगड़ाई ली और करवट बदल ली \\n जरा देर में उसके खर्राटों की आवाजें सुनायी देने लगीं \\n ऐसा जान पड़ा कि वह गहरी निद्रा में मग्न हो गया है \\n आध घंटे तक वह पड़ा सोता रहा और बेगमें ज्यों की त्यों सिर नीचा किये दीवार के चित्रों की भाँति खड़ी रहीं \\n उनमें दो एक महिलाएँ जो ढीठ थीं  घूँघट की ओट से नादिरशाह को देख भी रही थीं और आपस में दबी जबान में कानाफूसी कर रही थीं  कैसा भयंकर स्वरूप है \\n कितनी रणोन्मत्त आँखें हैं \\n कितना भारी शरीर है \\n आदमी काहे को है  देव है\\nसहसा नादिरशाह की आँखें खुल गयीं \\n परियों का दल पूर्ववत् खड़ा था \\n उसे जागते देखकर बेगमों ने सिर नीचे कर लिये और अंग समेटकर भेंड़ों की भाँति एक दूसरे से मिल गयीं \\n सबके दिल धड़क रहे थे कि अब यह जालिम नाचने गाने को कहेगा  तब कैसे क्या होगा \\n खुदा इस जालिम से समझे \\n मगर नाचा तो न जायगा \\n चाहे जान ही क्यों न जाय \\n इससे ज्यादा जिल्लत अब न सही जायगी\\nसहसा नादिरशाह कठोर शब्दों में बोला  ऐ खुदा की बंदियो  मैंने तुम्हारा इम्तहान लेने के लिए बुलाया था और अफसोस के साथ कहना पड़ता है कि तुम्हारी निसबत मेरा जो गुमान था  वह हर्फ ब हर्फ सच निकला \\n जब किसी कौम की औरतों में ग़ैरत नहीं रहती तो वह कौम मुरदा हो जाती है\\nदेखना चाहता था कि तुम लोगों में अभी कुछ ग़ैरत बाकी है या नहीं \\n इसलिए मैंने तुम्हें यहाँ बुलाया था \\n मैं तुम्हारी बेहुरमती नहीं करना चाहता था \\n मैं इतना ऐश का बंदा नहीं हूँ  वरना आज भेड़ों के गल्ले चराता होता \\n न इतना हवसपरस्त हूँ  वरना आज फारस में सरोद और सितार की ताने सुनता होता  जिसका मजा मैं हिंदुस्तानी गाने से कहीं ज्यादा उठा सकता हूँ \\n मुझे सिर्फ तुम्हारा इम्तहान लेना था \\n मुझे यह देखकर सच्चा मलाल हो रहा है कि तुममें ग़ैरत का जौहर बाकी न रहा \\n क्या यह मुमकिन न था कि तुम मेरे हुक्म को पैरों तले कुचल देतीं \\n जब तुम यहाँ आ गयीं तो मैंने तुम्हें एक और मौका दिया \\n मैंने नींद का बहाना किया \\n क्या यह मुमकिन न था कि तुममें से कोई खुदा की बंदी इस कटार को उठाकर मेरे जिगर में चुभा देती \\n मैं कलामेपाक की कसम खाकर कहता हूँ कि तुम में से किसी को कटार पर हाथ रखते देखकर मुझे बेहद खुशी होती  मैं उन नाजुक हाथों के सामने गरदन झुका देता \\n पर अफसोस है कि आज तैमूरी खानदान की एक बेटी भी यहाँ ऐसी नहीं निकली जो अपनी हुरमत बिगाड़ने पर हाथ उठाती \\n अब यह सल्तनत जिंदा नहीं रह सकती \\n इसकी हस्ती के दिन गिने हुए हैं \\n इसका निशान बहुत जल्द दुनिया से मिट जाएगा \\n तुम लोग जाओ और हो सके तो अब भी सल्तनत को बचाओ वरना इसी तरह हवस की गुलामी करते हुए दुनिया से रुखसत हो जाओगी\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./pariksha.txt\"\n",
    "\n",
    "# Open the file with encoding set to UTF-8 (for Hindi text)\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    hindi_text = file.read()\n",
    "\n",
    "hindi_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a304d",
   "metadata": {},
   "source": [
    "## Preprocessing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d986e41",
   "metadata": {},
   "source": [
    "we get the data from the hindi corpus https://github.com/gayatrivenugopal/Hindi-Aesthetics-Corpus/tree/master\n",
    "\n",
    "which has stories from some of the most prominent hindi authors of India\n",
    "\n",
    "Luckily the text is preprocessed for the most part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "709eaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# hindi_text = hindi_text.replace('\\n', ' ')\n",
    "\n",
    "# hindi_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ad0f3",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "We leverage indic-nlp library for tokenizing purposes, which is developed by AI4Bharat (a venture by IIT Madras) the documentation can be found [here](https://pypi.org/project/indic-nlp-library/). AI4Bharat has also fine tuned [fasttext](https://fasttext.cc/docs/en/support.html) for Hindi and made public access to the model and [embeddings](https://indicnlp.ai4bharat.org/pages/fasttext/) which we leverage going forward for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a28725d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136077aa",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "Now that we have the clean data, we can tokenize using the ```indic-nlp``` library that provides an API to tokenize based on [sentences](https://indic-nlp-library.readthedocs.io/en/latest/indicnlp.tokenize.html#sentence-tokenize-module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b773eb4",
   "metadata": {},
   "source": [
    "## Tokenize based on sentences\n",
    "\n",
    "since our sentences are separated by \\n escape sequence we can just split based on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0c728133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है ',\n",
       " ' गलियों में खून की नदियाँ बह रही हैं ',\n",
       " ' चारों तरफ हाहाकार मचा हुआ है ',\n",
       " ' बाजार बंद हैं ',\n",
       " ' दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं ',\n",
       " ' किसी की जान सलामत नहीं है ',\n",
       " ' कहीं घरों में आग लगी हुई है  कहीं बाजार लुट रहा है  कोई किसी की फरियाद नहीं सुनता ',\n",
       " ' रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है ',\n",
       " ' ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती ']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = hindi_text.split('\\n')\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a5a71",
   "metadata": {},
   "source": [
    "For better string parsing later on, we make the 1st element of ```sentences``` in line with other elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0bee772d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है ',\n",
       " ' गलियों में खून की नदियाँ बह रही हैं ',\n",
       " ' चारों तरफ हाहाकार मचा हुआ है ',\n",
       " ' बाजार बंद हैं ',\n",
       " ' दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं ',\n",
       " ' किसी की जान सलामत नहीं है ',\n",
       " ' कहीं घरों में आग लगी हुई है  कहीं बाजार लुट रहा है  कोई किसी की फरियाद नहीं सुनता ',\n",
       " ' रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है ',\n",
       " ' ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती ']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentences[0] = ' '+sentences[0]\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972625a",
   "metadata": {},
   "source": [
    "LGTM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc06b7",
   "metadata": {},
   "source": [
    "### Average length of each sentence\n",
    "\n",
    "We will need this later to see how many ngrams to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f1d640bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.6063829787234"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_length = 0\n",
    "currSum = 0\n",
    "for sentence in sentences:\n",
    "#     print(sentence)\n",
    "    currSum += len(sentence)\n",
    "average_length = currSum/len(sentences)\n",
    "\n",
    "average_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742fb44",
   "metadata": {},
   "source": [
    "Something is fishy. The length of every sentence seems to be around 8-9 but we get a value of 62.\n",
    "\n",
    "That's because the len() in python calculates the number of bytes in the string rather than actual number of characters. Therefore we use a library ```indic-nlp-library``` for tokenizes purposes. \n",
    "\n",
    "Therefore, we leverage indic-nlp library for tokenizing purposes, which is developed by AI4Bharat (a venture by IIT Madras) the documentation can be found [here](https://pypi.org/project/indic-nlp-library/). AI4Bharat has also fine tuned [fasttext](https://fasttext.cc/docs/en/support.html) for Hindi and made public access to the model and [embeddings](https://indicnlp.ai4bharat.org/pages/fasttext/) which we leverage going forward for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "140ae266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.202127659574469"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "avg_len = 0\n",
    "curr_sum = 0\n",
    "for sentence in sentences:\n",
    "    words = indic_tokenize.trivial_tokenize(sentence, lang='hi')\n",
    "    curr_sum += len(words)\n",
    "\n",
    "avg_len = curr_sum / len(sentences)\n",
    "\n",
    "avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f30d27",
   "metadata": {},
   "source": [
    "This looks more realistic. LGTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1a28d545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है  SENTENCE_END',\n",
       " 'SENTENCE_START  गलियों में खून की नदियाँ बह रही हैं  SENTENCE_END',\n",
       " 'SENTENCE_START  चारों तरफ हाहाकार मचा हुआ है  SENTENCE_END',\n",
       " 'SENTENCE_START  बाजार बंद हैं  SENTENCE_END',\n",
       " 'SENTENCE_START  दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं  SENTENCE_END',\n",
       " 'SENTENCE_START  किसी की जान सलामत नहीं है  SENTENCE_END',\n",
       " 'SENTENCE_START  कहीं घरों में आग लगी हुई है  कहीं बाजार लुट रहा है  कोई किसी की फरियाद नहीं सुनता  SENTENCE_END',\n",
       " 'SENTENCE_START  रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है  SENTENCE_END',\n",
       " 'SENTENCE_START  ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती  SENTENCE_END']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentences = []\n",
    "for sentence in sentences:\n",
    "#     words = sentence.split(' ')\n",
    "#     print(sentence)\n",
    "    # filtered_words = [word for word in words if word not in stop_words_hindi]\n",
    "    sentence_with_markers = 'SENTENCE_START ' + sentence + ' SENTENCE_END'\n",
    "    filtered_sentence = ' '.join(sentence_with_markers)\n",
    "    filtered_sentences.append(sentence_with_markers)\n",
    "\n",
    "filtered_sentences[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "63251ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['नादिरशाह', 'की', 'सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा', 'है']\n",
      "['गलियों', 'में', 'खून', 'की', 'नदियाँ', 'बह', 'रही', 'हैं']\n",
      "['चारों', 'तरफ', 'हाहाकार', 'मचा', 'हुआ', 'है']\n",
      "['बाजार', 'बंद', 'हैं']\n",
      "['दिल्ली', 'के', 'लोग', 'घरों', 'के', 'द्वार', 'बंद', 'किये', 'जान', 'की', 'खैर', 'मना', 'रहे', 'हैं']\n",
      "['किसी', 'की', 'जान', 'सलामत', 'नहीं', 'है']\n",
      "['कहीं', 'घरों', 'में', 'आग', 'लगी', 'हुई', 'है', 'कहीं', 'बाजार', 'लुट', 'रहा', 'है', 'कोई', 'किसी', 'की', 'फरियाद', 'नहीं', 'सुनता']\n",
      "['रईसों', 'की', 'बेगमें', 'महलों', 'से', 'निकाली', 'जा', 'रही', 'हैं', 'और', 'उनकी', 'बेहुरमती', 'की', 'जाती', 'है']\n",
      "['ईरानी', 'सिपाहियों', 'की', 'रक्तपिपासा', 'किसी', 'तरह', 'नहीं', 'बुझती']\n",
      "['मानव', 'हृदय', 'की', 'क्रूरता', 'कठोरता', 'और', 'पैशाचिकता', 'अपना', 'विकरालतम', 'रूप', 'धारण', 'किये', 'हुए', 'हैं']\n",
      "['इसी', 'समय', 'नादिरशाह', 'ने', 'बादशाही', 'महल', 'में', 'प्रवेश', 'किया']\n",
      "['दिल्ली', 'उन', 'दिनों', 'भोगविलास', 'की', 'केंद्र', 'बनी', 'हुई', 'थी']\n",
      "['सजावट', 'और', 'तकल्लुफ', 'के', 'सामानों', 'से', 'रईसों', 'के', 'भवन', 'भरे', 'रहते', 'थे']\n",
      "['स्त्रियों', 'को', 'बनाव', 'सिंगार', 'के', 'सिवा', 'कोई', 'काम', 'न', 'था']\n",
      "['पुरुषों', 'को', 'सुख', 'भोग', 'के', 'सिवा', 'और', 'कोई', 'चिंता', 'न', 'थी']\n",
      "['राजनीति', 'का', 'स्थान', 'शेरो', 'शायरी', 'ने', 'ले', 'लिया', 'था']\n",
      "['समस्त', 'प्रान्तों', 'से', 'धन', 'खिंच', 'खिंचकर', 'दिल्ली', 'आता', 'था', 'और', 'पानी', 'की', 'भाँति', 'बहाया', 'जाता', 'था']\n",
      "['वेश्याओं', 'की', 'चाँदी', 'थी']\n",
      "['कहीं', 'तीतरों', 'के', 'जोड़', 'होते', 'थे', 'कहीं', 'बटेरों', 'और', 'बुलबुलों', 'की', 'पालियाँ', 'ठनती', 'थीं']\n",
      "['सारा', 'नगर', 'विलास', 'निद्रा', 'में', 'मग्न', 'था']\n",
      "['नादिरशाह', 'शाही', 'महल', 'में', 'पहुँचा', 'तो', 'वहाँ', 'का', 'सामान', 'देखकर', 'उसकी', 'आँखें', 'खुल', 'गयीं']\n",
      "['उसका', 'जन्म', 'दरिद्र', 'घर', 'में', 'हुआ', 'था']\n",
      "['उसका', 'समस्त', 'जीवन', 'रणभूमि', 'में', 'ही', 'कटा', 'था']\n",
      "['भोगविलास', 'का', 'उसे', 'चसका', 'न', 'लगा', 'था']\n",
      "['कहाँ', 'रणक्षेत्र', 'के', 'कष्ट', 'और', 'कहाँ', 'यह', 'सुख', 'साम्राज्य']\n",
      "['जिधर', 'आँख', 'उठती', 'थी', 'उधर', 'से', 'हटने', 'का', 'नाम', 'न', 'लेती', 'थी']\n",
      "['संध्या', 'हो', 'गयी', 'थी']\n",
      "['नादिरशाह', 'अपने', 'सरदारों', 'के', 'साथ', 'महल', 'की', 'सैर', 'करता', 'और', 'अपनी', 'पसंद', 'की', 'चीजों', 'को', 'बटोरता', 'हुआ', 'दीवाने', 'खास', 'में', 'आकर', 'कारचोबी', 'मसनद', 'पर', 'बैठ', 'गया', 'सरदारों', 'को', 'वहाँ', 'से', 'चले', 'जाने', 'का', 'हुक्म', 'दे', 'दिया', 'अपने', 'सब', 'हथियार', 'खोलकर', 'रख', 'दिये', 'और', 'महल', 'में', 'द़रोगा', 'को', 'बुलाकर', 'हुक्म', 'दिया', 'मैं', 'शाही', 'बेगमों', 'का', 'नाच', 'देखना', 'चाहता', 'हूँ']\n",
      "['तुम', 'इसी', 'वक्त', 'उनको', 'सुंदर', 'वस्त्रभूषणों', 'से', 'सजाकर', 'मेरे', 'सामने', 'लाओ']\n",
      "['खबरदार', 'जरा', 'भी', 'देर', 'न', 'हो']\n",
      "['मैं', 'कोई', 'उज्र', 'या', 'इनकार', 'नहीं', 'सुन', 'सकता']\n",
      "['दारोगा', 'ने', 'यह', 'नादिरशाही', 'हुक्म', 'सुना', 'तो', 'होश', 'उड़', 'गये']\n",
      "['वह', 'महिलाएँ', 'जिन', 'पर', 'कभी', 'सूर्य', 'की', 'दृष्टि', 'भी', 'नहीं', 'पड़ी', 'कैसे', 'इस', 'मजलिस', 'में', 'आयेंगी']\n",
      "['नाचने', 'का', 'तो', 'कहना', 'ही', 'क्या']\n",
      "['शाही', 'बेगमों', 'का', 'इतना', 'अपमान', 'कभी', 'न', 'हुआ', 'था']\n",
      "['हा', 'नरपिशाच']\n",
      "['दिल्ली', 'को', 'खून', 'से', 'रँगकर', 'भी', 'तेरा', 'चित्त', 'शांत', 'नहीं', 'हुआ']\n",
      "['मगर', 'नादिरशाह', 'के', 'सम्मुख', 'एक', 'शब्द', 'भी', 'जबान', 'से', 'निकालना', 'अग्नि', 'के', 'मुख', 'में', 'कूदना', 'था']\n",
      "['सिर', 'झुकाकर', 'आदाब', 'बजा', 'लाया', 'और', 'आकर', 'रनिवास', 'में', 'सब', 'बेगमों', 'को', 'नादिरशाही', 'हुक्म', 'सुना', 'दिया', 'उसके', 'साथ', 'ही', 'यह', 'इत्तला', 'भी', 'दे', 'दी', 'कि', 'जरा', 'भी', 'ताम्मुल', 'न', 'हो', 'नादिरशाह', 'कोई', 'उज्र', 'या', 'हीला', 'न', 'सुनेगा']\n",
      "['शाही', 'खानदान', 'पर', 'इतनी', 'बड़ी', 'विपत्ति', 'कभी', 'नहीं', 'पड़ी', 'पर', 'इस', 'समय', 'विजयी', 'बादशाह', 'की', 'आज्ञा', 'को', 'शिरोधार्य', 'करने', 'के', 'सिवा', 'प्राण', 'रक्षा', 'का', 'अन्य', 'कोई', 'उपाय', 'नहीं', 'था']\n",
      "['बेगमों', 'ने', 'यह', 'आज्ञा', 'सुनी', 'तो', 'हतबुद्धि', 'सी', 'हो', 'गयीं']\n",
      "['सारे', 'रनिवास', 'में', 'मातम', 'सा', 'छा', 'गया']\n",
      "['वह', 'चहल', 'पहल', 'गायब', 'हो', 'गयी']\n",
      "['सैकड़ों', 'हृदयों', 'से', 'इस', 'अत्याचारी', 'के', 'प्रति', 'एक', 'शाप', 'निकल', 'गया']\n",
      "['किसी', 'ने', 'आकाश', 'की', 'ओर', 'सहायतायाचक', 'लोचनों', 'से', 'देखा', 'किसी', 'ने', 'खुदा', 'और', 'रसूल', 'को', 'सुमिरन', 'किया', 'पर', 'ऐसी', 'एक', 'महिला', 'भी', 'न', 'थी', 'जिसकी', 'निगाह', 'कटार', 'या', 'तलवार', 'की', 'तरफ', 'गयी', 'हो']\n",
      "['यद्यपि', 'इनमें', 'कितनी', 'ही', 'बेगमों', 'के', 'नसों', 'में', 'राजपूतानियों', 'का', 'रक्त', 'प्रवाहित', 'हो', 'रहा', 'था', 'पर', 'इंद्रियलिप्सा', 'ने', 'जौहर', 'की', 'पुरानी', 'आग', 'ठंडी', 'कर', 'दी', 'थी']\n",
      "['सुखभोग', 'की', 'लालसा', 'आत्म', 'सम्मान', 'का', 'सर्वनाश', 'कर', 'देती', 'है']\n",
      "['आपस', 'में', 'सलाह', 'करके', 'मर्यादा', 'की', 'रक्षा', 'का', 'कोई', 'उपाय', 'सोचने', 'की', 'मुहलत', 'न', 'थी']\n",
      "['एक', 'एक', 'पल', 'भाग्य', 'का', 'निर्णय', 'कर', 'रहा', 'था']\n",
      "['हताश', 'होकर', 'सभी', 'ललनाओं', 'ने', 'पापी', 'के', 'सम्मुख', 'जाने', 'का', 'निश्चय', 'किया']\n",
      "['आँखों', 'से', 'आँसू', 'जारी', 'थे', 'दिलों', 'से', 'आहें', 'निकल', 'रही', 'थीं', 'पर', 'रत्न', 'जटित', 'आभूषण', 'पहने', 'जा', 'रहे', 'थे', 'अश्रु', 'सिंचित', 'नेत्रों', 'में', 'सुरमा', 'लगाया', 'जा', 'रहा', 'था', 'और', 'शोक', 'व्यथित', 'हृदयों', 'पर', 'सुगंध', 'का', 'लेप', 'किया', 'जा', 'रहा', 'था']\n",
      "['कोई', 'केश', 'गूँथती', 'थी', 'कोई', 'माँगों', 'में', 'मोतियाँ', 'पिरोती', 'थी']\n",
      "['एक', 'भी', 'ऐसे', 'पक्के', 'इरादे', 'की', 'स्त्री', 'न', 'थी', 'जो', 'ईश्वर', 'पर', 'अथवा', 'अपनी', 'टेक', 'पर', 'इस', 'आज्ञा', 'का', 'उल्लंघन', 'करने', 'का', 'साहस', 'कर', 'सके']\n",
      "['एक', 'घंटा', 'भी', 'न', 'गुजरने', 'पाया', 'था', 'कि', 'बेगमात', 'पूरे', 'के', 'पूरे', 'आभूषणों', 'से', 'जगमगाती', 'अपने', 'मुख', 'की', 'कांति', 'से', 'बेले', 'और', 'गुलाब', 'की', 'कलियों', 'को', 'लजाती', 'सुगंध', 'की', 'लपटें', 'उड़ाती', 'छमछम', 'करती', 'हुई', 'दीवाने', 'खास', 'में', 'आकर', 'नादिरशाह', 'के', 'सामने', 'खड़ी', 'हो', 'गयीं']\n",
      "['नादिरशाह', 'ने', 'एक', 'बार', 'कनखियों', 'से', 'परियों', 'के', 'इस', 'दल', 'को', 'देखा', 'और', 'तब', 'मसनद', 'की', 'टेक', 'लगाकर', 'लेट', 'गया']\n",
      "['अपनी', 'तलवार', 'और', 'कटार', 'सामने', 'रख', 'दीं']\n",
      "['एक', 'क्षण', 'में', 'उसकी', 'आँखें', 'झपकने', 'लगीं']\n",
      "['उसने', 'एक', 'अँगड़ाई', 'ली', 'और', 'करवट', 'बदल', 'ली']\n",
      "['जरा', 'देर', 'में', 'उसके', 'खर्राटों', 'की', 'आवाजें', 'सुनायी', 'देने', 'लगीं']\n",
      "['ऐसा', 'जान', 'पड़ा', 'कि', 'वह', 'गहरी', 'निद्रा', 'में', 'मग्न', 'हो', 'गया', 'है']\n",
      "['आध', 'घंटे', 'तक', 'वह', 'पड़ा', 'सोता', 'रहा', 'और', 'बेगमें', 'ज्यों', 'की', 'त्यों', 'सिर', 'नीचा', 'किये', 'दीवार', 'के', 'चित्रों', 'की', 'भाँति', 'खड़ी', 'रहीं']\n",
      "['उनमें', 'दो', 'एक', 'महिलाएँ', 'जो', 'ढीठ', 'थीं', 'घूँघट', 'की', 'ओट', 'से', 'नादिरशाह', 'को', 'देख', 'भी', 'रही', 'थीं', 'और', 'आपस', 'में', 'दबी', 'जबान', 'में', 'कानाफूसी', 'कर', 'रही', 'थीं', 'कैसा', 'भयंकर', 'स्वरूप', 'है']\n",
      "['कितनी', 'रणोन्मत्त', 'आँखें', 'हैं']\n",
      "['कितना', 'भारी', 'शरीर', 'है']\n",
      "['आदमी', 'काहे', 'को', 'है', 'देव', 'है']\n",
      "['सहसा', 'नादिरशाह', 'की', 'आँखें', 'खुल', 'गयीं']\n",
      "['परियों', 'का', 'दल', 'पूर्ववत्', 'खड़ा', 'था']\n",
      "['उसे', 'जागते', 'देखकर', 'बेगमों', 'ने', 'सिर', 'नीचे', 'कर', 'लिये', 'और', 'अंग', 'समेटकर', 'भेंड़ों', 'की', 'भाँति', 'एक', 'दूसरे', 'से', 'मिल', 'गयीं']\n",
      "['सबके', 'दिल', 'धड़क', 'रहे', 'थे', 'कि', 'अब', 'यह', 'जालिम', 'नाचने', 'गाने', 'को', 'कहेगा', 'तब', 'कैसे', 'क्या', 'होगा']\n",
      "['खुदा', 'इस', 'जालिम', 'से', 'समझे']\n",
      "['मगर', 'नाचा', 'तो', 'न', 'जायगा']\n",
      "['चाहे', 'जान', 'ही', 'क्यों', 'न', 'जाय']\n",
      "['इससे', 'ज्यादा', 'जिल्लत', 'अब', 'न', 'सही', 'जायगी']\n",
      "['सहसा', 'नादिरशाह', 'कठोर', 'शब्दों', 'में', 'बोला', 'ऐ', 'खुदा', 'की', 'बंदियो', 'मैंने', 'तुम्हारा', 'इम्तहान', 'लेने', 'के', 'लिए', 'बुलाया', 'था', 'और', 'अफसोस', 'के', 'साथ', 'कहना', 'पड़ता', 'है', 'कि', 'तुम्हारी', 'निसबत', 'मेरा', 'जो', 'गुमान', 'था', 'वह', 'हर्फ', 'ब', 'हर्फ', 'सच', 'निकला']\n",
      "['जब', 'किसी', 'कौम', 'की', 'औरतों', 'में', 'ग़ैरत', 'नहीं', 'रहती', 'तो', 'वह', 'कौम', 'मुरदा', 'हो', 'जाती', 'है']\n",
      "['देखना', 'चाहता', 'था', 'कि', 'तुम', 'लोगों', 'में', 'अभी', 'कुछ', 'ग़ैरत', 'बाकी', 'है', 'या', 'नहीं']\n",
      "['इसलिए', 'मैंने', 'तुम्हें', 'यहाँ', 'बुलाया', 'था']\n",
      "['मैं', 'तुम्हारी', 'बेहुरमती', 'नहीं', 'करना', 'चाहता', 'था']\n",
      "['मैं', 'इतना', 'ऐश', 'का', 'बंदा', 'नहीं', 'हूँ', 'वरना', 'आज', 'भेड़ों', 'के', 'गल्ले', 'चराता', 'होता']\n",
      "['न', 'इतना', 'हवसपरस्त', 'हूँ', 'वरना', 'आज', 'फारस', 'में', 'सरोद', 'और', 'सितार', 'की', 'ताने', 'सुनता', 'होता', 'जिसका', 'मजा', 'मैं', 'हिंदुस्तानी', 'गाने', 'से', 'कहीं', 'ज्यादा', 'उठा', 'सकता', 'हूँ']\n",
      "['मुझे', 'सिर्फ', 'तुम्हारा', 'इम्तहान', 'लेना', 'था']\n",
      "['मुझे', 'यह', 'देखकर', 'सच्चा', 'मलाल', 'हो', 'रहा', 'है', 'कि', 'तुममें', 'ग़ैरत', 'का', 'जौहर', 'बाकी', 'न', 'रहा']\n",
      "['क्या', 'यह', 'मुमकिन', 'न', 'था', 'कि', 'तुम', 'मेरे', 'हुक्म', 'को', 'पैरों', 'तले', 'कुचल', 'देतीं']\n",
      "['जब', 'तुम', 'यहाँ', 'आ', 'गयीं', 'तो', 'मैंने', 'तुम्हें', 'एक', 'और', 'मौका', 'दिया']\n",
      "['मैंने', 'नींद', 'का', 'बहाना', 'किया']\n",
      "['क्या', 'यह', 'मुमकिन', 'न', 'था', 'कि', 'तुममें', 'से', 'कोई', 'खुदा', 'की', 'बंदी', 'इस', 'कटार', 'को', 'उठाकर', 'मेरे', 'जिगर', 'में', 'चुभा', 'देती']\n",
      "['मैं', 'कलामेपाक', 'की', 'कसम', 'खाकर', 'कहता', 'हूँ', 'कि', 'तुम', 'में', 'से', 'किसी', 'को', 'कटार', 'पर', 'हाथ', 'रखते', 'देखकर', 'मुझे', 'बेहद', 'खुशी', 'होती', 'मैं', 'उन', 'नाजुक', 'हाथों', 'के', 'सामने', 'गरदन', 'झुका', 'देता']\n",
      "['पर', 'अफसोस', 'है', 'कि', 'आज', 'तैमूरी', 'खानदान', 'की', 'एक', 'बेटी', 'भी', 'यहाँ', 'ऐसी', 'नहीं', 'निकली', 'जो', 'अपनी', 'हुरमत', 'बिगाड़ने', 'पर', 'हाथ', 'उठाती']\n",
      "['अब', 'यह', 'सल्तनत', 'जिंदा', 'नहीं', 'रह', 'सकती']\n",
      "['इसकी', 'हस्ती', 'के', 'दिन', 'गिने', 'हुए', 'हैं']\n",
      "['इसका', 'निशान', 'बहुत', 'जल्द', 'दुनिया', 'से', 'मिट', 'जाएगा']\n",
      "['तुम', 'लोग', 'जाओ', 'और', 'हो', 'सके', 'तो', 'अब', 'भी', 'सल्तनत', 'को', 'बचाओ', 'वरना', 'इसी', 'तरह', 'हवस', 'की', 'गुलामी', 'करते', 'हुए', 'दुनिया', 'से', 'रुखसत', 'हो', 'जाओगी']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "for sentence in filtered_sentences:\n",
    "    words = sentence.split(' ')\n",
    "    # Remove SENTENCE_START tokens\n",
    "    words_without_markers = [word for word in words if word not in\n",
    "                             ['SENTENCE_START', 'SENTENCE_END', '']]\n",
    "    tokenized_sentences.append(words_without_markers)\n",
    "\n",
    "# Print tokenized sentences without markers\n",
    "for sentence in tokenized_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a4771",
   "metadata": {},
   "source": [
    "### Using Ngrams to find the most frequent sequence of words used\n",
    "As we start on to create the training data set for our model, in sequence modeling ngrams play an important role. As we might be aware, the data we input is sequence of words and the predicting label would be the next word of the sequence, and we'll create similar pairs of two words, three words and so on. In short, this is what ngrams are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e80aebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 890 µs, sys: 986 µs, total: 1.88 ms\n",
      "Wall time: 2.67 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('न', 'थी'), 4),\n",
       " (('रहा', 'था'), 4),\n",
       " (('था', 'कि'), 4),\n",
       " (('महल', 'में'), 3),\n",
       " (('के', 'सिवा'), 3),\n",
       " (('न', 'था'), 3),\n",
       " (('था', 'और'), 3),\n",
       " (('की', 'भाँति'), 3),\n",
       " (('रही', 'थीं'), 3),\n",
       " (('है', 'कि'), 3)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "bigram_counts = Counter(ngrams(hindi_text.split(), 2))\n",
    "bigram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3b2d2",
   "metadata": {},
   "source": [
    "Similarly, we find ngram counts for n =[3,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0c420bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('निद्रा', 'में', 'मग्न'), 2),\n",
       " (('आँखें', 'खुल', 'गयीं'), 2),\n",
       " (('दीवाने', 'खास', 'में'), 2),\n",
       " (('खास', 'में', 'आकर'), 2),\n",
       " (('शाही', 'बेगमों', 'का'), 2),\n",
       " (('कोई', 'उज्र', 'या'), 2),\n",
       " (('नादिरशाही', 'हुक्म', 'सुना'), 2),\n",
       " (('जा', 'रहा', 'था'), 2),\n",
       " (('था', 'कि', 'तुम'), 2),\n",
       " (('हूँ', 'वरना', 'आज'), 2)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts = Counter(ngrams(hindi_text.split(), 3))\n",
    "trigram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9c63b7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('दीवाने', 'खास', 'में', 'आकर'), 2),\n",
       " (('क्या', 'यह', 'मुमकिन', 'न'), 2),\n",
       " (('यह', 'मुमकिन', 'न', 'था'), 2),\n",
       " (('मुमकिन', 'न', 'था', 'कि'), 2),\n",
       " (('नादिरशाह', 'की', 'सेना', 'ने'), 1),\n",
       " (('की', 'सेना', 'ने', 'दिल्ली'), 1),\n",
       " (('सेना', 'ने', 'दिल्ली', 'में'), 1),\n",
       " (('ने', 'दिल्ली', 'में', 'कत्लेआम'), 1),\n",
       " (('दिल्ली', 'में', 'कत्लेआम', 'कर'), 1),\n",
       " (('में', 'कत्लेआम', 'कर', 'रखा'), 1)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadgram_counts = Counter(ngrams(hindi_text.replace('&', 'i').split(), 4))\n",
    "quadgram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c1b6dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('क्या', 'यह', 'मुमकिन', 'न', 'था'), 2),\n",
       " (('यह', 'मुमकिन', 'न', 'था', 'कि'), 2),\n",
       " (('नादिरशाह', 'की', 'सेना', 'ने', 'दिल्ली'), 1),\n",
       " (('की', 'सेना', 'ने', 'दिल्ली', 'में'), 1),\n",
       " (('सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम'), 1),\n",
       " (('ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर'), 1),\n",
       " (('दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा'), 1),\n",
       " (('में', 'कत्लेआम', 'कर', 'रखा', 'है'), 1),\n",
       " (('कत्लेआम', 'कर', 'रखा', 'है', 'गलियों'), 1),\n",
       " (('कर', 'रखा', 'है', 'गलियों', 'में'), 1)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pentagram_counts = Counter(ngrams(hindi_text.split(), 5))\n",
    "pentagram_counts.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8800e95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('क्या', 'यह', 'मुमकिन', 'न', 'था', 'कि'), 2),\n",
       " (('नादिरशाह', 'की', 'सेना', 'ने', 'दिल्ली', 'में'), 1),\n",
       " (('की', 'सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम'), 1),\n",
       " (('सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर'), 1),\n",
       " (('ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा'), 1),\n",
       " (('दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा', 'है'), 1),\n",
       " (('में', 'कत्लेआम', 'कर', 'रखा', 'है', 'गलियों'), 1),\n",
       " (('कत्लेआम', 'कर', 'रखा', 'है', 'गलियों', 'में'), 1),\n",
       " (('कर', 'रखा', 'है', 'गलियों', 'में', 'खून'), 1),\n",
       " (('रखा', 'है', 'गलियों', 'में', 'खून', 'की'), 1)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hexagram_counts = Counter(ngrams(hindi_text.split(), 6))\n",
    "hexagram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "11f60cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('नादिरशाह', 'की', 'सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम'), 1),\n",
       " (('की', 'सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर'), 1),\n",
       " (('सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा'), 1),\n",
       " (('ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा', 'है'), 1),\n",
       " (('दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा', 'है', 'गलियों'), 1),\n",
       " (('में', 'कत्लेआम', 'कर', 'रखा', 'है', 'गलियों', 'में'), 1),\n",
       " (('कत्लेआम', 'कर', 'रखा', 'है', 'गलियों', 'में', 'खून'), 1),\n",
       " (('कर', 'रखा', 'है', 'गलियों', 'में', 'खून', 'की'), 1),\n",
       " (('रखा', 'है', 'गलियों', 'में', 'खून', 'की', 'नदियाँ'), 1),\n",
       " (('है', 'गलियों', 'में', 'खून', 'की', 'नदियाँ', 'बह'), 1)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heptagram_counts = Counter(ngrams(hindi_text.split(), 7))\n",
    "heptagram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b94447",
   "metadata": {},
   "source": [
    "## Creating the training dataset\n",
    "\n",
    "We create the training dataset using the following approach:\n",
    "\n",
    "- First we add words which occur at the start of the sentence\n",
    "- We then add ngrams by creating ngrams from n = 2 to 15 (because avg length of our sentences were 13\n",
    "- then we add end of sentence words\n",
    "\n",
    "First we find the first word of each sentence\n",
    "\n",
    "### Adding start words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "95ff13a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'नादिरशाह',\n",
       " 'गलियों',\n",
       " 'चारों',\n",
       " 'बाजार',\n",
       " 'दिल्ली',\n",
       " 'किसी',\n",
       " 'कहीं',\n",
       " 'रईसों',\n",
       " 'ईरानी']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_words = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = indic_tokenize.trivial_tokenize(sentence, lang='hi')\n",
    "    first_words.append(words[0])\n",
    "\n",
    "# filtered_sentences[0:10]\n",
    "first_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "969808cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('नादिरशाह', 4),\n",
       " ('मैं', 4),\n",
       " ('एक', 4),\n",
       " ('दिल्ली', 3),\n",
       " ('', 2),\n",
       " ('किसी', 2),\n",
       " ('कहीं', 2),\n",
       " ('उसका', 2),\n",
       " ('तुम', 2),\n",
       " ('वह', 2)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_counts = Counter(first_words)\n",
    "first_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0c92ddb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "46390996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('', 2), ('नादिरशाह', 4), ('गलियों', 1), ('चारों', 1), ('बाजार', 1), ('दिल्ली', 3), ('किसी', 2), ('कहीं', 2), ('रईसों', 1), ('ईरानी', 1), ('मानव', 1), ('इसी', 1), ('सजावट', 1), ('स्त्रियों', 1), ('पुरुषों', 1), ('राजनीति', 1), ('समस्त', 1), ('वेश्याओं', 1), ('सारा', 1), ('उसका', 2), ('भोगविलास', 1), ('कहाँ', 1), ('जिधर', 1), ('संध्या', 1), ('तुम', 2), ('खबरदार', 1), ('मैं', 4), ('दारोगा', 1), ('वह', 2), ('नाचने', 1), ('शाही', 2), ('हा', 1), ('मगर', 2), ('सिर', 1), ('बेगमों', 1), ('सारे', 1), ('सैकड़ों', 1), ('यद्यपि', 1), ('सुखभोग', 1), ('आपस', 1), ('एक', 4), ('हताश', 1), ('आँखों', 1), ('कोई', 1), ('अपनी', 1), ('उसने', 1), ('जरा', 1), ('ऐसा', 1), ('आध', 1), ('उनमें', 1), ('कितनी', 1), ('कितना', 1), ('आदमी', 1), ('सहसा', 2), ('परियों', 1), ('उसे', 1), ('सबके', 1), ('खुदा', 1), ('चाहे', 1), ('इससे', 1), ('जब', 2), ('देखना', 1), ('इसलिए', 1), ('न', 1), ('मुझे', 2), ('क्या', 2), ('मैंने', 1), ('पर', 1), ('अब', 1), ('इसकी', 1), ('इसका', 1)])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_counts.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772184dc",
   "metadata": {},
   "source": [
    "Now we add the first words in our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b45421a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_start_token='SENTENCE_START'\n",
    "\n",
    "X_train = [[sentence_start_token]*c for sent,c in first_word_counts.items()]\n",
    "y_train = [[sent]*c for sent,c in first_word_counts.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2f64cdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START', 'SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START'],\n",
       "  ['SENTENCE_START']],\n",
       " [['', ''],\n",
       "  ['नादिरशाह', 'नादिरशाह', 'नादिरशाह', 'नादिरशाह'],\n",
       "  ['गलियों'],\n",
       "  ['चारों'],\n",
       "  ['बाजार'],\n",
       "  ['दिल्ली', 'दिल्ली', 'दिल्ली'],\n",
       "  ['किसी', 'किसी'],\n",
       "  ['कहीं', 'कहीं'],\n",
       "  ['रईसों'],\n",
       "  ['ईरानी'],\n",
       "  ['मानव'],\n",
       "  ['इसी'],\n",
       "  ['सजावट'],\n",
       "  ['स्त्रियों'],\n",
       "  ['पुरुषों'],\n",
       "  ['राजनीति'],\n",
       "  ['समस्त'],\n",
       "  ['वेश्याओं'],\n",
       "  ['सारा'],\n",
       "  ['उसका', 'उसका'],\n",
       "  ['भोगविलास'],\n",
       "  ['कहाँ'],\n",
       "  ['जिधर'],\n",
       "  ['संध्या'],\n",
       "  ['तुम', 'तुम'],\n",
       "  ['खबरदार'],\n",
       "  ['मैं', 'मैं', 'मैं', 'मैं'],\n",
       "  ['दारोगा'],\n",
       "  ['वह', 'वह'],\n",
       "  ['नाचने'],\n",
       "  ['शाही', 'शाही'],\n",
       "  ['हा'],\n",
       "  ['मगर', 'मगर'],\n",
       "  ['सिर'],\n",
       "  ['बेगमों'],\n",
       "  ['सारे'],\n",
       "  ['सैकड़ों'],\n",
       "  ['यद्यपि'],\n",
       "  ['सुखभोग'],\n",
       "  ['आपस'],\n",
       "  ['एक', 'एक', 'एक', 'एक'],\n",
       "  ['हताश'],\n",
       "  ['आँखों'],\n",
       "  ['कोई'],\n",
       "  ['अपनी'],\n",
       "  ['उसने'],\n",
       "  ['जरा'],\n",
       "  ['ऐसा'],\n",
       "  ['आध'],\n",
       "  ['उनमें'],\n",
       "  ['कितनी'],\n",
       "  ['कितना'],\n",
       "  ['आदमी'],\n",
       "  ['सहसा', 'सहसा'],\n",
       "  ['परियों'],\n",
       "  ['उसे'],\n",
       "  ['सबके'],\n",
       "  ['खुदा'],\n",
       "  ['चाहे'],\n",
       "  ['इससे'],\n",
       "  ['जब', 'जब'],\n",
       "  ['देखना'],\n",
       "  ['इसलिए'],\n",
       "  ['न'],\n",
       "  ['मुझे', 'मुझे'],\n",
       "  ['क्या', 'क्या'],\n",
       "  ['मैंने'],\n",
       "  ['पर'],\n",
       "  ['अब'],\n",
       "  ['इसकी'],\n",
       "  ['इसका']])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "24106add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 71)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44191ab",
   "metadata": {},
   "source": [
    "We use Fisher Yates Algorithm to shuffle our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c72a6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fisher_yates (arr1, arr2):\n",
    "\n",
    "    # We will Start from the last element\n",
    "    # and swap one by one.\n",
    "    n = len(arr1)\n",
    "    if n != len(arr2):\n",
    "        return None\n",
    "\n",
    "    for i in range(n - 1, 0, -1):\n",
    "\n",
    "        # Pick a random index from 0 to i\n",
    "        j = random.randint(0, i)\n",
    "        #print(i, j)\n",
    "\n",
    "        # Swap arr[i] with the element at random index\n",
    "        arr1[i], arr1[j] = arr1[j], arr1[i]\n",
    "        arr2[i], arr2[j] = arr2[j], arr2[i]\n",
    "\n",
    "    return arr1, arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b1fd2fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 71)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = fisher_yates(X_train, y_train)\n",
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "32507d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['जरा'], ['आदमी'], ['जिधर'], ['वह', 'वह'], ['इसी'], ['कहाँ'], ['खबरदार'], ['कितना'], ['उसका', 'उसका'], ['इससे'], ['मैंने'], ['नाचने'], ['मगर', 'मगर'], ['बेगमों'], ['कितनी'], ['कहीं', 'कहीं'], ['उनमें'], ['सुखभोग'], ['सारे'], ['समस्त'], ['सजावट'], ['उसने'], ['दारोगा'], ['न'], ['सबके'], ['सैकड़ों'], ['राजनीति'], ['एक', 'एक', 'एक', 'एक'], ['मुझे', 'मुझे'], ['जब', 'जब'], ['अपनी'], ['इसलिए'], ['रईसों'], ['चाहे'], ['मैं', 'मैं', 'मैं', 'मैं'], ['तुम', 'तुम'], ['नादिरशाह', 'नादिरशाह', 'नादिरशाह', 'नादिरशाह'], ['खुदा'], ['इसका'], ['भोगविलास'], ['हा'], ['किसी', 'किसी'], ['उसे'], ['वेश्याओं'], ['सिर'], ['मानव'], ['ऐसा'], ['पुरुषों'], ['सहसा', 'सहसा'], ['ईरानी'], ['इसकी'], ['चारों'], ['देखना'], ['शाही', 'शाही'], ['बाजार'], ['दिल्ली', 'दिल्ली', 'दिल्ली'], ['स्त्रियों'], ['कोई'], ['सारा'], ['आँखों'], ['क्या', 'क्या'], ['संध्या'], ['अब'], ['हताश'], ['पर'], ['परियों'], ['आपस'], ['यद्यपि'], ['गलियों'], ['आध'], ['', '']]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d87d4",
   "metadata": {},
   "source": [
    "### Create Vocabulary and word indexing\n",
    "\n",
    "Here we create a vocabulary based on the words we have in our corpus, and then we create an indexing for each of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c60abb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 593 unique words tokens.\n",
      "The least frequent word in our vocabulary is 'रुखसत' and appeared 1 times.\n",
      "Using vocabulary size 593.\n",
      "\n",
      "Example sentence: 'SENTENCE_START नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है  SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'नादिरशाह', 'की', 'सेना', 'ने', 'दिल्ली', 'में', 'कत्लेआम', 'कर', 'रखा', 'है', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Tokenize the sentences into words, making sure to remove end-of-sentence period\n",
    "tokenized_sentences = [nltk.word_tokenize(sent.replace('.', '')) for sent in filtered_sentences]\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "# sentence_end_token = \"SENTENCE_END\"\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(  \"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "\n",
    "vocabulary_size = len(word_freq.items())\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "#for i, sent in enumerate(tokenized_sentences):\n",
    "#    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "vocabulary_size = len(word_freq.items())\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "\n",
    "print(  \"\\nExample sentence: '%s'\" % filtered_sentences[1])\n",
    "print(  \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aedb03",
   "metadata": {},
   "source": [
    "We Flatten our list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "713d2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item for sublist in X_train for item in sublist]\n",
    "y_train = [item for sublist in y_train for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b01de",
   "metadata": {},
   "source": [
    "We encode the above to tokens based on the word_to_index we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a2e1892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = [[word_to_index[symbol]] for symbol,word in zip(X_train, y_train) if word in word_to_index]\n",
    "y_tokens = [[word_to_index[word]] for symbol,word in zip(X_train, y_train) if word in word_to_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9336eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tokens\n",
    "y_train = y_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "19cddbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 92)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc17b85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0], [0], [0], [0], [0]], [[74], [474], [269], [33], [33]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5], y_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3469e3",
   "metadata": {},
   "source": [
    "### Adding all ngrams to our training dataset\n",
    "\n",
    "We find ngrams up to 15 (as the average length of our sentences were 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "97edcc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram- 2 length: 1150\n",
      "ngram- 3 length: 1223\n",
      "ngram- 4 length: 1232\n",
      "ngram- 5 length: 1233\n",
      "ngram- 6 length: 1233\n",
      "ngram- 7 length: 1233\n",
      "ngram- 8 length: 1232\n",
      "ngram- 9 length: 1231\n",
      "ngram- 10 length: 1230\n",
      "ngram- 11 length: 1229\n",
      "ngram- 12 length: 1228\n",
      "ngram- 13 length: 1227\n",
      "ngram- 14 length: 1226\n",
      "ngram- 15 length: 1225\n"
     ]
    }
   ],
   "source": [
    "ngrams_up_to_15 = []\n",
    "for i in range(2, 16):\n",
    "    ngram_counts = Counter(ngrams(hindi_text.split(), i))\n",
    "    print('ngram-', i, 'length:', len(ngram_counts))\n",
    "    ngrams_up_to_15.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5db6e",
   "metadata": {},
   "source": [
    "Sanity check: we check the 10 most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "11758b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('न', 'थी'), 4),\n",
       " (('रहा', 'था'), 4),\n",
       " (('था', 'कि'), 4),\n",
       " (('महल', 'में'), 3),\n",
       " (('के', 'सिवा'), 3),\n",
       " (('न', 'था'), 3),\n",
       " (('था', 'और'), 3),\n",
       " (('की', 'भाँति'), 3),\n",
       " (('रही', 'थीं'), 3),\n",
       " (('है', 'कि'), 3)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_up_to_15[0].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf2449",
   "metadata": {},
   "source": [
    "Now we extend our X_train, y_train by shuffline and appending all ngrams up to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b246ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 13/13 [00:00<00:00, 143.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1, len(ngrams_up_to_15))):\n",
    "    ngrams_to_learn = ngrams_up_to_15[i]\n",
    "    X_train_2 = [[word_to_index[w] for w in sent[0][:-1]] for sent in (ngrams_to_learn.most_common())\n",
    "                   if all([w in word_to_index for w in sent[0]])]\n",
    "    y_train_2 = [[word_to_index[w] for w in sent[0][1:]] for sent in (ngrams_to_learn.most_common())\n",
    "                   if all([w in word_to_index for w in sent[0]])]\n",
    "    X_train_2 = X_train_2[:2000]\n",
    "    y_train_2 = y_train_2[:2000]\n",
    "    X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "    X_train.extend(X_train_2)\n",
    "    y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "09859305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16061, 16061)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eefc26",
   "metadata": {},
   "source": [
    "### Trying to learn how to end sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268247f0",
   "metadata": {},
   "source": [
    "A random tokenized sample from our training set looks as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a898386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([203, 204, 2, 205, 206, 7, 207, 208, 209, 210, 211, 61], [204, 2, 205, 206, 7, 207, 208, 209, 210, 211, 61, 64]), ([315, 12, 36, 129, 18, 5, 130, 13, 316, 17], [12, 36, 129, 18, 5, 130, 13, 316, 17, 131]), ([20, 32, 3, 6, 27, 10], [32, 3, 6, 27, 10, 57]), ([42, 51, 144, 7, 57, 53, 117, 436, 13, 437, 3, 105, 50], [51, 144, 7, 57, 53, 117, 436, 13, 437, 3, 105, 50, 438]), ([7, 167, 5, 70, 128, 509, 11, 20, 168, 510, 511, 58, 512], [167, 5, 70, 128, 509, 11, 20, 168, 510, 511, 58, 512, 4]), ([487, 488, 489, 62], [488, 489, 62, 39]), ([6, 391, 141, 35, 40, 15, 392, 393], [391, 141, 35, 40, 15, 392, 393, 394]), ([3, 213, 38, 34, 98, 214, 99, 2], [213, 38, 34, 98, 214, 99, 2, 215]), ([38, 34, 98, 214, 99, 2, 215], [34, 98, 214, 99, 2, 215, 216]), ([479, 49, 31, 19, 77], [49, 31, 19, 77, 480])]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(zip(X_train, y_train)), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "29ef6ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c6f5a668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENTENCE_START', 'मानव', 'हृदय', 'की', 'क्रूरता', 'कठोरता', 'और', 'पैशाचिकता', 'अपना', 'विकरालतम', 'रूप', 'धारण', 'किये', 'हुए', 'हैं', 'SENTENCE_END'] ['SENTENCE_END', 'हैं', 'हुए', 'किये', 'धारण', 'रूप', 'विकरालतम', 'अपना', 'पैशाचिकता', 'और', 'कठोरता', 'क्रूरता', 'की', 'हृदय', 'मानव', 'SENTENCE_START']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences[10], tokenized_sentences[10][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "61940b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['दीवार', 'के', 'चित्रों', 'की', 'भाँति', 'खड़ी', 'रहीं', 'SENTENCE_END'], ['अन्य', 'कोई', 'उपाय', 'नहीं', 'था', 'SENTENCE_END'], ['और', 'बुलबुलों', 'की', 'पालियाँ', 'ठनती', 'थीं', 'SENTENCE_END'], ['उसका', 'जन्म', 'दरिद्र', 'घर', 'में', 'हुआ', 'था', 'SENTENCE_END'], ['बेहुरमती', 'नहीं', 'करना', 'चाहता', 'था', 'SENTENCE_END'], ['जिन', 'पर', 'कभी', 'सूर्य', 'की', 'दृष्टि', 'भी', 'नहीं', 'पड़ी', 'कैसे', 'इस', 'मजलिस', 'में', 'आयेंगी', 'SENTENCE_END'], ['SENTENCE_START', 'अब', 'यह', 'सल्तनत', 'जिंदा', 'नहीं', 'रह', 'सकती', 'SENTENCE_END'], ['बाजार', 'बंद', 'हैं', 'SENTENCE_END'], ['ज्यों', 'की', 'त्यों', 'सिर', 'नीचा', 'किये', 'दीवार', 'के', 'चित्रों', 'की', 'भाँति', 'खड़ी', 'रहीं', 'SENTENCE_END'], ['पापी', 'के', 'सम्मुख', 'जाने', 'का', 'निश्चय', 'किया', 'SENTENCE_END']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "last_n_words = []\n",
    "for i in range(3, 16):\n",
    "    tokenized_sentences_90 = random.sample(list(tokenized_sentences), 90)\n",
    "    for s in tokenized_sentences_90:\n",
    "        last_n_words.append(s[::-1][:i][::-1])\n",
    "\n",
    "print(random.sample(last_n_words, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c90716f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "406bf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eos = [[word_to_index[w] for w in sent[:-1]] for sent in last_n_words\n",
    "                         if all([w in word_to_index for w in sent])]\n",
    "y_train_eos = [[word_to_index[w] for w in sent[1:]] for sent in last_n_words\n",
    "                         if all([w in word_to_index for w in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6058a60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16061, 16061)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ff6d9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_eos)\n",
    "y_train.extend(y_train_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "17811963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17219, 17219)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54156a",
   "metadata": {},
   "source": [
    "After appening the end of sentence tokens, our training dataset is of about 17k instances\n",
    "\n",
    "Pickle our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4e780275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path where you want to save the pickle file\n",
    "file_path = 'X_train.pickle'\n",
    "\n",
    "# Open the file in binary write mode and save the array\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(X_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6f92e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'y_train.pickle'\n",
    "\n",
    "# Open the file in binary write mode and save the array\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(y_train, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402629e",
   "metadata": {},
   "source": [
    "## Starting with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea28b4",
   "metadata": {},
   "source": [
    "### Loading the embeddings\n",
    "\n",
    "As mentioned above, we will use the indicnlp fast text embeddings to convert our text to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "30bc60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हिन्दी विकि डीवीडी परियोजना -0.078365 0.049841 -0.23186 0.072641 -0.10667 0.15986 -0.036957 -0.039915 -0.17246 0.070029 0.16518 0.092211 -0.1706 -0.11882 0.11917 0.0092635 -0.20663 0.041592 -0.10816 0.06173 0.13963 0.23366 -0.060042 0.30493 0.14802 0.029197 -0.10574 0.034834 0.18872 0.038719 -0.036717 0.21376 -0.0070195 -0.14029 0.071023 -0.18044 0.016243 -0.095747 -0.097626 -0.068943 0.059256 -0.12054 -0.065286 -0.24157 0.036331 -0.14777 -0.22512 0.05606 0.0035369 -0.2494 0.047777 0.024651 -0.013128 -0.23639 0.038466 0.062365 -0.11298 -0.067375 0.099474 0.011086 -0.062023 -0.060376 0.10178 -0.0066213 0.17612 -0.014129 -0.053058 0.24642 -0.0038288 -0.30613 -0.24535 -0.23448 -0.079723 -0.010486 0.068892 0.084526 0.06854 -0.13237 0.10715 -0.14325 -0.17184 0.20093 0.16292 0.14244 0.21243 0.03807 -0.21012 -0.31086 -0.021485 0.12033 0.060714 -0.027038 0.20331 0.030021 -0.39067 -0.0098092 0.15615 0.0041575 0.17791 -0.081739 0.11193 -0.053256 0.10047 0.011498 -0.088826 0.20969 -0.040423 0.24449 0.066236 -0.082111 0.036556 -0.039248 -0.02011 -0.12246 -0.069376 0.032821 0.12677 0.039534 -0.092619 0.1807 0.22061 -0.039044 -0.22346 -0.13551 -0.065507 0.038692 0.14319 0.092603 -0.045684 -0.0047913 -0.0079866 0.0060002 -0.1402 -0.069391 -0.0019816 0.004622 -0.22528 -0.26915 -0.0096326 0.39776 -0.11617 0.13252 -0.13893 0.18866 -0.075462 0.080481 0.088816 -0.1501 -0.23335 -0.0088571 -0.029746 -0.084251 0.046373 -0.019399 0.11205 -0.14402 0.0032386 0.20223 -0.22877 -0.24094 0.10437 -0.10931 0.26135 -0.13482 -0.16121 -0.037952 0.17203 -0.13757 -0.0013978 0.30631 -0.05499 0.066195 -0.15927 0.268 0.095519 0.093642 0.083128 -0.18578 0.14099 -0.13505 -0.21431 0.26425 -0.17616 0.17065 -0.073169 0.20672 0.26876 -0.25248 -0.076466 -0.02241 0.16161 -0.1787 0.04891 -0.074224 0.33418 -0.10822 0.27853 0.011056 -0.23107 0.052632 0.043238 0.093672 -0.013565 -0.0043547 -0.13003 -0.25392 -0.38329 0.11827 -0.28815 0.028203 -0.22723 0.07816 0.23039 -0.012378 0.056551 -0.1395 -0.0028789 0.25031 0.15897 0.018888 -0.13251 0.1725 0.10357 -0.18051 -0.046143 -0.17101 -0.1258 0.083772 0.10192 0.06628 0.022771 0.20813 -0.033328 -0.2406 -0.051467 -0.00095692 0.25995 -0.054842 0.013706 0.00024795 0.061175 0.06304 -0.042725 -0.10456 0.20379 -0.14827 0.23111 0.17068 0.18913 -0.10744 -0.17335 -0.02538 -0.070823 0.034547 -0.10387 0.041082 -0.084267 -0.050602 0.060069 -0.017042 0.22878 0.0042483 -0.0798 -0.10257 -0.1143 0.38138 0.24482 0.11387 -0.08282 -0.13242 -0.0042191 0.29918 0.20509 -0.23409 -0.13579 0.011493 0.22479 0.11025 0.09081 0.10672 0.19877 -0.035146 -0.042073 0.00075463 -0.0042505 0.036672 -0.088108 0.0091228 0.030616 -0.23643 -0.069865 -0.11101 0.022933 -0.10471 0.067176 -0.23911 -0.21566 -0.10519 -0.34334 -0.099426 \n",
      "\n",
      "Found 50797 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as  np\n",
    "\n",
    "indicft = \"./wiki.hi/wiki.hi.vec\"\n",
    "\n",
    "embeddings_index = {} #initialize dictionary\n",
    "f = open(indicft, encoding='utf8')\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # print(values)\n",
    "        word = values[0]\n",
    "        # print(word)\n",
    "        if isinstance(values[1],float):\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        else:\n",
    "            coefs = np.asarray(values[2:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "except:\n",
    "    print(line)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c818d162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index[\"साथ\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ff91006d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0782e-01, -1.9232e-01, -3.0594e-01,  1.4494e-02,  1.7182e-01,\n",
       "        1.6094e-01,  6.4644e-02, -7.0808e-02, -6.4341e-02, -6.7175e-03,\n",
       "       -4.8330e-02, -1.1570e-01, -1.6334e-01,  4.9730e-02, -9.6842e-02,\n",
       "        2.8898e-02, -3.8412e-02,  1.4467e-01,  7.3464e-02,  2.7375e-01,\n",
       "       -1.1724e-01,  1.2058e-01,  3.2511e-01,  2.4432e-01, -1.4156e-01,\n",
       "        1.6366e-03, -1.6643e-01,  2.6620e-01,  4.1349e-01, -4.6825e-02,\n",
       "       -7.1790e-02, -9.0043e-02,  1.0646e-01,  7.5878e-02,  2.3027e-02,\n",
       "       -1.2405e-01,  2.9869e-01, -2.3322e-02, -3.2134e-02, -1.2343e-02,\n",
       "        8.4678e-02, -1.3602e-01, -2.1371e-01,  2.5373e-01, -1.4988e-01,\n",
       "        2.6918e-02, -1.1517e-01, -2.8798e-02,  1.9614e-01,  9.8374e-02,\n",
       "        1.8497e-01,  3.5065e-02,  1.6455e-01,  1.5761e-02, -1.4590e-01,\n",
       "        5.1624e-02,  1.7907e-02, -1.3496e-01,  4.5411e-02, -2.2380e-02,\n",
       "        1.1144e-01,  4.7230e-02, -2.9735e-02, -1.3784e-01, -1.0852e-01,\n",
       "       -1.4252e-02,  2.8834e-02,  1.9384e-01,  9.5036e-02, -1.5727e-01,\n",
       "       -5.8992e-02,  1.7812e-01,  2.2024e-01,  1.3250e-01,  2.4579e-01,\n",
       "        3.1041e-02,  5.8686e-02,  1.5902e-01, -4.8639e-02, -1.2784e-01,\n",
       "       -1.4559e-01,  1.7328e-01, -9.4999e-02, -5.4506e-02,  2.3753e-01,\n",
       "       -1.2036e-01, -1.2767e-02, -9.9538e-02,  2.3780e-02, -1.2118e-01,\n",
       "        1.9322e-02,  1.0466e-01, -1.7284e-01, -2.2186e-01,  2.5376e-01,\n",
       "       -2.9373e-01,  1.5500e-01,  1.2445e-01,  5.8802e-02,  2.2198e-01,\n",
       "       -4.3650e-02,  2.4641e-01,  3.8854e-02, -7.2470e-03,  2.9747e-02,\n",
       "       -2.7017e-02,  1.1658e-01,  4.8017e-02,  3.5570e-02,  1.1102e-01,\n",
       "        4.7579e-02,  7.4829e-02, -4.4614e-03, -8.3154e-02, -1.5906e-01,\n",
       "        1.1278e-01, -1.7087e-01, -1.3157e-01,  1.7132e-03, -9.1355e-02,\n",
       "       -2.0537e-01, -1.1903e-01,  6.4825e-02,  1.1056e-01, -1.4461e-01,\n",
       "        1.0351e-01, -9.5190e-02, -5.8336e-02, -2.0200e-01,  2.4015e-01,\n",
       "        2.3337e-02,  4.4650e-02,  1.7548e-01,  1.7211e-01,  7.3793e-03,\n",
       "        5.1138e-02,  1.0113e-02, -3.7472e-01, -1.1296e-01,  2.4984e-02,\n",
       "        8.2675e-02, -4.1725e-04,  1.1220e-01,  2.1502e-01,  9.4338e-02,\n",
       "        1.8785e-02,  1.5562e-01,  3.2905e-02, -3.7410e-02, -1.2896e-01,\n",
       "       -2.4280e-01,  5.6197e-02, -7.4340e-02,  2.3067e-01,  8.7123e-02,\n",
       "        9.2267e-02,  1.4916e-02,  3.1206e-02, -2.0051e-01,  1.7819e-01,\n",
       "        5.4258e-02, -5.3778e-02, -2.4118e-02, -7.7283e-02,  6.7971e-02,\n",
       "       -1.9087e-01,  1.0364e-01, -2.9634e-04,  2.5416e-01, -1.7701e-01,\n",
       "        1.4644e-01,  6.0591e-02, -2.1140e-01, -1.1191e-02, -5.8210e-02,\n",
       "        6.4361e-02, -8.3507e-02,  2.2831e-01,  9.3740e-02, -5.1769e-02,\n",
       "        1.0789e-02, -4.2101e-02,  1.6874e-01,  1.0629e-01,  2.5312e-01,\n",
       "        6.7141e-02,  8.5437e-02, -3.2167e-01,  2.8337e-01, -3.6508e-02,\n",
       "        4.3823e-02,  1.6934e-01, -1.5991e-01,  1.5867e-01, -2.7706e-01,\n",
       "        2.1802e-01,  3.2309e-01, -6.3113e-02,  8.4441e-02,  2.1507e-01,\n",
       "        1.9253e-01,  1.9849e-01,  1.4805e-01, -1.7080e-01, -3.2801e-02,\n",
       "        4.9308e-02, -3.8784e-02, -1.5787e-01,  1.1655e-01, -3.5386e-01,\n",
       "        3.5482e-01,  3.3542e-01, -9.1740e-03,  1.7458e-01, -1.2580e-01,\n",
       "        2.4397e-01, -5.3089e-02,  3.8018e-02, -2.0305e-02, -2.3638e-01,\n",
       "        1.2557e-01, -1.2010e-02, -8.0747e-02,  1.6885e-01, -9.4269e-02,\n",
       "        8.0051e-02, -7.0744e-02, -7.8829e-02, -4.5631e-02,  8.5142e-02,\n",
       "        1.0083e-01, -1.1528e-01, -1.8251e-02,  1.2337e-02, -6.7736e-02,\n",
       "        2.1110e-01, -8.0785e-02, -1.1645e-01,  2.3301e-01, -6.9616e-02,\n",
       "       -1.1478e-01,  1.1949e-01,  4.7202e-02, -1.6883e-02, -1.6434e-01,\n",
       "        1.4471e-01, -1.3769e-01,  9.3942e-02,  2.4447e-01, -1.3924e-01,\n",
       "        8.2368e-02, -1.4621e-01,  1.6285e-01, -8.9248e-02,  1.7219e-02,\n",
       "       -1.5782e-01,  5.3635e-02, -2.4357e-01,  1.0732e-01,  1.7016e-01,\n",
       "        8.9615e-03, -7.2837e-03,  6.4703e-02,  5.4099e-02,  3.1339e-01,\n",
       "        9.8165e-02, -7.2037e-02, -8.5761e-02, -1.8656e-01,  1.8913e-01,\n",
       "       -2.2340e-01,  3.3754e-01, -3.4913e-01, -6.2489e-02, -1.4200e-01,\n",
       "        1.0193e-01,  7.7511e-02,  2.2283e-01,  7.8880e-02,  1.6957e-01,\n",
       "       -5.5324e-02, -4.0789e-02, -1.2362e-01,  4.8947e-02, -3.3826e-02,\n",
       "       -7.0512e-02,  7.7694e-02,  1.0608e-01, -2.0611e-02, -1.9094e-02,\n",
       "       -7.1949e-02, -1.0360e-01,  4.9611e-02,  4.2959e-01, -4.0947e-03,\n",
       "        3.4238e-02,  8.3021e-03, -6.5769e-02, -1.0765e-01], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"साथ\"][:299]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db6e4b",
   "metadata": {},
   "source": [
    "As we see above, our embedding matrix has a dimension of [vocabulary_size, 299]. 299 is the size of embeddings for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "163b188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_dim = 299\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4bc1537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in vocab:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector[:299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0102293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 299)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7de86fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('समस्त', 2)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe31de9",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d78cb",
   "metadata": {},
   "source": [
    "## Building the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1d2f50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "\n",
    "        # Randomly initialize the network parameters\n",
    "        #self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        #self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, embedding_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "\n",
    "        # Set Indic embeddings matrix\n",
    "        self.G = embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7475c7",
   "metadata": {},
   "source": [
    "Implementing forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7ea10284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # sometimes, may want to do this first:\n",
    "    #x = np.vectorize(round)(x)\n",
    "\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "76e48d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # embedding of x[t]:\n",
    "        e_t = self.G[x[t]]\n",
    "\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        #s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        s[t] = np.tanh(self.U.dot(e_t) + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "\n",
    "    return [o, s]\n",
    "\n",
    "RNN.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d7c16c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 299)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "656f5bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d6555",
   "metadata": {},
   "source": [
    "We prototype our feedforward ops and ensure our dimensions are aight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ecd2f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299,) (593, 299)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((593,),\n",
       " array([0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634, 0.00168634, 0.00168634,\n",
       "        0.00168634, 0.00168634, 0.00168634]))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dim = vocabulary_size\n",
    "hidden_dim = 299\n",
    "embedding_dim = 299\n",
    "U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, embedding_dim))\n",
    "W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "x = np.random.randint(0, high=word_dim, size=word_dim)\n",
    "T = len(x)\n",
    "s = np.zeros((T + 1, hidden_dim))\n",
    "s_m1 = np.zeros(hidden_dim)\n",
    "o = np.zeros((T, word_dim))\n",
    "e_0 = embedding_matrix[x[0]]\n",
    "s_0 = np.tanh(U.dot(e_0) + W.dot(s_m1))\n",
    "print(s_0.shape, V.shape)\n",
    "o_0 = softmax(V.dot(s_0))\n",
    "o_0.shape, o_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa975691",
   "metadata": {},
   "source": [
    "### We implement the predict function, which returns the next word with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cd977264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o[-1], axis=1)\n",
    "\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09316989",
   "metadata": {},
   "source": [
    "In the above predict, we assume that we feed only sequence, it returns the last output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "74c4fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNN.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497c37d",
   "metadata": {},
   "source": [
    "In the above predict method, we try to find the best output of all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fe8efad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "में सरोद\n",
      "[3, 531]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train[1000]]), X_train[1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e64bd827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "बहाना किया क्या यह मुमकिन न था कि तुममें से\n",
      "[549, 38, 55, 22, 176, 9, 4, 20, 175, 6]\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train[10000]]), X_train[10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91490bd9",
   "metadata": {},
   "source": [
    "### Trying the model, and predicting for X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2d15d6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17219"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ec30a11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, [549, 38, 55, 22, 176, 9, 4, 20, 175, 6])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, X_train[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "38607268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 593) [[0.00168634 0.00168634 0.00168634 ... 0.00168634 0.00168634 0.00168634]\n",
      " [0.00168634 0.00168634 0.00168634 ... 0.00168634 0.00168634 0.00168634]\n",
      " [0.00168634 0.00168634 0.00168634 ... 0.00168634 0.00168634 0.00168634]\n",
      " ...\n",
      " [0.00172975 0.00169986 0.00167878 ... 0.00177008 0.00165001 0.00172158]\n",
      " [0.00164581 0.00163014 0.00161869 ... 0.00166414 0.00165275 0.00170269]\n",
      " [0.00169951 0.00173443 0.0017908  ... 0.00164089 0.00175766 0.00162179]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "model = RNN(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10000])\n",
    "print (o.shape, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "085fb4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(o[-1], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d2a3e",
   "metadata": {},
   "source": [
    "The words of the predicted sequence are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8b293165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) [  0   0   0   0   0 263 283 503 408  62]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10000])\n",
    "print(predictions.shape, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b022833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START SENTENCE_START SENTENCE_START SENTENCE_START SENTENCE_START कटा बैठ शब्दों पिरोती रहे\n"
     ]
    }
   ],
   "source": [
    "print (\"x:\\n%s\" % (\" \".join([index_to_word[x] for x in predictions])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab867938",
   "metadata": {},
   "source": [
    "### Calculating the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4076fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNN.calculate_total_loss = calculate_total_loss\n",
    "RNN.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "27b477bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 6.385194\n",
      "Actual loss: 6.385601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/hkfvt6cd7yxb1tq43fx_kb5c0000gn/T/ipykernel_98884/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    }
   ],
   "source": [
    "#  Limit to 1000 examples to save time\n",
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eae42f",
   "metadata": {},
   "source": [
    "### Training backpropagation through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "dd0ce4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            #dLdU[:,x[bptt_step]] += delta_t\n",
    "            dLdU += np.outer(delta_t, self.G[x[bptt_step]])\n",
    "\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd780dd3",
   "metadata": {},
   "source": [
    "### Gradient Checking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ce0c90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient) / (\n",
    "                                np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "\n",
    "               # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print( \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print( \"+h Loss: %f\" % gradplus)\n",
    "                print( \"-h Loss: %f\" % gradminus)\n",
    "                print( \"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print( \"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print( \"Relative Error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "\n",
    "        print( \"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNN.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "390859fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 2990.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ccba2",
   "metadata": {},
   "source": [
    "### Stocastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "155ae16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNN.sgd_step = numpy_sdg_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bdc5f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "\n",
    "    for epoch in range(nepoch):\n",
    "\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9820eaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9b3feaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13 ms ± 926 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "model = RNN(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[1000], y_train[1000], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd60b4",
   "metadata": {},
   "source": [
    "Running for 100 rows data, for 10 Epochs (Just trying out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "28f547f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-24 03:12:48: Loss after num_examples_seen=0 epoch=0: 6.387678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/hkfvt6cd7yxb1tq43fx_kb5c0000gn/T/ipykernel_98884/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-24 03:12:48: Loss after num_examples_seen=100 epoch=1: 6.378936\n",
      "2024-02-24 03:12:49: Loss after num_examples_seen=200 epoch=2: 6.370090\n",
      "2024-02-24 03:12:49: Loss after num_examples_seen=300 epoch=3: 6.360844\n",
      "2024-02-24 03:12:49: Loss after num_examples_seen=400 epoch=4: 6.350897\n",
      "2024-02-24 03:12:49: Loss after num_examples_seen=500 epoch=5: 6.339937\n",
      "2024-02-24 03:12:50: Loss after num_examples_seen=600 epoch=6: 6.327633\n",
      "2024-02-24 03:12:50: Loss after num_examples_seen=700 epoch=7: 6.313629\n",
      "2024-02-24 03:12:50: Loss after num_examples_seen=800 epoch=8: 6.297540\n",
      "2024-02-24 03:12:51: Loss after num_examples_seen=900 epoch=9: 6.278953\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNN(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[2000:2100], y_train[2000:2100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9421f",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "We have the model ready. Its time to predict the next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d5d25e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Repeat until we get an end token and keep our sentences to less than senten_max_length words for now\n",
    "    while (not new_sentence[-1] == word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        \n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            \n",
    "            # correcting for abnormalities\n",
    "            #abs_v = [-i if i <0 else i for i in next_word_probs[-1][0]] \n",
    "            #nrm_v = [i/sum(abs_v) for i in abs_v] \n",
    "            #abs_v = [0 if i <0 else i for i in next_word_probs[-1][0]] \n",
    "            #abs_v = [0 if i <0 else i for i in next_word_probs[0][-1]] \n",
    "            #nrm_v = [i/sum(abs_v) for i in abs_v] \n",
    "            #samples = np.random.multinomial(1, nrm_v)\n",
    "            #sampled_word = np.argmax(samples)\n",
    "            \n",
    "            # the secret sauce of creativity\n",
    "            samples = np.random.multinomial(1, next_word_probs[0][-1])\n",
    "            \n",
    "            sampled_word = np.argmax(samples)\n",
    "            \n",
    "        new_sentence.append(sampled_word)\n",
    "\n",
    "    print(new_sentence)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    #print(sentence_str)\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5d8580a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 52, 212, 385, 167, 395, 190, 185, 514, 554, 271, 164, 121, 542, 91]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['दिया',\n",
       " 'बादशाही',\n",
       " 'पापी',\n",
       " 'अफसोस',\n",
       " 'पहने',\n",
       " 'खैर',\n",
       " 'बह',\n",
       " 'सच',\n",
       " 'कलामेपाक',\n",
       " 'उठती',\n",
       " 'तुम्हारा',\n",
       " 'सकता',\n",
       " 'पैरों']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senten_max_length = 15\n",
    "generate_sentence(model, senten_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "38c41d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 211, 420, 488, 564, 402, 308, 379, 81, 56, 326, 310, 209, 423, 229, 143, 420, 251, 264, 68]\n",
      "धारण बेगमात दिल गरदन व्यथित आयेंगी भाग्य वरना खुदा हीला हा विकरालतम कांति चिंता ऐसी बेगमात पालियाँ चसका\n",
      "[0, 584, 514, 500, 96, 558, 321, 430, 313, 49, 471, 206, 428, 417, 245, 323, 295, 225, 10, 483]\n",
      "मिट सच सही तरह रखते आदाब छमछम तेरा देखकर कितना कठोरता लपटें घंटा चाँदी लाया सजाकर सिंगार को\n",
      "[0, 56, 547, 71, 318, 268, 409, 182, 387, 347, 202, 143, 270, 449, 165, 176, 120, 126, 245, 459]\n",
      "खुदा मौका आकर अग्नि साम्राज्य ऐसे रखा आँखों अत्याचारी बुझती ऐसी आँख आध इम्तहान मुमकिन उज्र कैसे चाँदी\n",
      "[0, 488, 563, 251, 326, 237, 67, 529, 494, 536, 11, 369, 4, 509, 589, 274, 37, 559, 253, 86]\n",
      "दिल हाथों पालियाँ हीला धन भाँति हवसपरस्त जायगा हिंदुस्तानी है लालसा था पड़ता गुलामी नाम कहीं बेहद सारा\n",
      "[0, 529, 33, 0, 287, 390, 299, 402, 316, 492, 130, 118, 162, 294, 402, 356, 280, 75, 461, 280]\n",
      "हवसपरस्त वह SENTENCE_START दिये दिलों सुन व्यथित शब्द समझे सम्मुख देखना गाने वस्त्रभूषणों व्यथित महिला चीजों कभी ढीठ\n",
      "[0, 100, 504, 59, 312, 236, 590, 406, 391, 215, 98, 422, 586, 569, 97, 576, 505, 423, 254, 206]\n",
      "सुख बोला अब रँगकर प्रान्तों करते माँगों आहें केंद्र उन जगमगाती जाओ निकली समय इसकी ऐ कांति नगर\n",
      "[0, 408, 183, 337, 90, 256, 519, 489, 101, 179, 580, 495, 329, 287, 393, 224, 344, 221, 214, 303]\n",
      "पिरोती गलियों हतबुद्धि आग पहुँचा लोगों धड़क समस्त दुनिया इसका चाहे बड़ी दिये जटित बनाव पहल भरे दिनों\n",
      "[0, 405, 349, 207, 327, 295, 340, 56, 189, 156, 97, 444, 20, 236, 459, 333, 293, 564, 275, 215]\n",
      "गूँथती शाप पैशाचिकता सुनेगा सजाकर मातम खुदा द्वार तब समय आवाजें कि प्रान्तों उनमें शिरोधार्य सुंदर गरदन लेती\n",
      "[0, 555, 469, 135, 522, 512, 569, 319, 313, 304, 163, 248, 154, 52, 497, 387, 108, 360, 304, 523]\n",
      "कसम स्वरूप दी इसलिए गुमान निकली कूदना तेरा जिन ज्यादा होते परियों दिया जाय आँखों उसे इनमें जिन\n",
      "[0, 279, 234, 43, 567, 328, 541, 260, 105, 542, 305, 128, 458, 429, 49, 495, 174, 376, 574, 190]\n",
      "पसंद ले हुक्म तैमूरी इतनी मलाल घर उसकी पैरों सूर्य कहना रहीं उड़ाती देखकर चाहे होता सोचने रह\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = generate_sentence(model, senten_max_length)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c211760",
   "metadata": {},
   "source": [
    "Running for 30 epochs\n",
    "Okay so we start our serious training of the model now, we did our training on kaggle for it to be able to run overnight. It took ~1 hrs 30 mins for 30 epochs to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0b58c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/hkfvt6cd7yxb1tq43fx_kb5c0000gn/T/ipykernel_98884/1857118698.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-24 03:19:43: Loss after num_examples_seen=0 epoch=0: 6.385708\n",
      "2024-02-24 03:25:27: Loss after num_examples_seen=17219 epoch=1: 3.338647\n",
      "2024-02-24 03:29:17: Loss after num_examples_seen=34438 epoch=2: 2.897831\n",
      "2024-02-24 03:32:38: Loss after num_examples_seen=51657 epoch=3: 2.847428\n",
      "2024-02-24 03:36:11: Loss after num_examples_seen=68876 epoch=4: 2.802486\n",
      "2024-02-24 03:39:35: Loss after num_examples_seen=86095 epoch=5: 2.772969\n",
      "2024-02-24 03:42:47: Loss after num_examples_seen=103314 epoch=6: 2.762610\n",
      "2024-02-24 03:45:58: Loss after num_examples_seen=120533 epoch=7: 2.758935\n",
      "2024-02-24 03:49:08: Loss after num_examples_seen=137752 epoch=8: 2.744101\n",
      "2024-02-24 03:51:59: Loss after num_examples_seen=154971 epoch=9: 2.743447\n",
      "2024-02-24 03:54:54: Loss after num_examples_seen=172190 epoch=10: 2.735423\n",
      "2024-02-24 03:58:01: Loss after num_examples_seen=189409 epoch=11: 2.731460\n",
      "2024-02-24 04:01:03: Loss after num_examples_seen=206628 epoch=12: 2.718971\n",
      "2024-02-24 04:04:00: Loss after num_examples_seen=223847 epoch=13: 2.747668\n",
      "Setting learning rate to 0.002500\n",
      "2024-02-24 04:06:49: Loss after num_examples_seen=241066 epoch=14: 2.689312\n",
      "2024-02-24 04:09:36: Loss after num_examples_seen=258285 epoch=15: 2.685230\n",
      "2024-02-24 04:12:22: Loss after num_examples_seen=275504 epoch=16: 2.687981\n",
      "Setting learning rate to 0.001250\n",
      "2024-02-24 04:15:18: Loss after num_examples_seen=292723 epoch=17: 2.649547\n",
      "2024-02-24 04:18:06: Loss after num_examples_seen=309942 epoch=18: 2.648616\n",
      "2024-02-24 04:21:01: Loss after num_examples_seen=327161 epoch=19: 2.647646\n",
      "2024-02-24 04:24:06: Loss after num_examples_seen=344380 epoch=20: 2.647069\n",
      "2024-02-24 04:27:00: Loss after num_examples_seen=361599 epoch=21: 2.646834\n",
      "2024-02-24 04:30:04: Loss after num_examples_seen=378818 epoch=22: 2.646868\n",
      "Setting learning rate to 0.000625\n",
      "2024-02-24 04:33:03: Loss after num_examples_seen=396037 epoch=23: 2.609215\n",
      "2024-02-24 04:36:11: Loss after num_examples_seen=413256 epoch=24: 2.603763\n",
      "2024-02-24 04:39:10: Loss after num_examples_seen=430475 epoch=25: 2.600860\n",
      "2024-02-24 04:42:02: Loss after num_examples_seen=447694 epoch=26: 2.598671\n",
      "2024-02-24 04:45:20: Loss after num_examples_seen=464913 epoch=27: 2.596901\n",
      "2024-02-24 04:48:17: Loss after num_examples_seen=482132 epoch=28: 2.595396\n",
      "2024-02-24 04:51:05: Loss after num_examples_seen=499351 epoch=29: 2.594064\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNN(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train, y_train, nepoch=30, evaluate_loss_after=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8728a53",
   "metadata": {},
   "source": [
    "Let's try generating sentences after serious training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c50c69cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 52, 212, 385, 167, 395, 190, 185, 514, 554, 271, 164, 121, 542, 91, 211, 420, 488, 564, 402]\n",
      "दिया बादशाही पापी अफसोस पहने खैर बह सच कलामेपाक उठती तुम्हारा सकता पैरों सुनता धारण बेगमात दिल गरदन\n",
      "[0, 308, 379, 81, 56, 326, 310, 209, 423, 229, 143, 420, 251, 264, 68, 584, 514, 500, 96, 558]\n",
      "आयेंगी भाग्य वरना खुदा हीला हा विकरालतम कांति चिंता ऐसी बेगमात पालियाँ चसका गयी मिट सच सही तरह\n",
      "[0, 321, 430, 313, 49, 471, 206, 428, 417, 245, 323, 295, 225, 10, 122, 73, 126, 545, 1]\n",
      "आदाब छमछम तेरा देखकर कितना कठोरता लपटें घंटा चाँदी लाया सजाकर सिंगार को नादिरशाही मेरे कैसे देतीं\n",
      "[0, 216, 71, 318, 268, 409, 182, 387, 347, 202, 143, 270, 449, 165, 176, 120, 126, 245, 459, 488]\n",
      "बनी आकर अग्नि साम्राज्य ऐसे रखा आँखों अत्याचारी बुझती ऐसी आँख आध इम्तहान मुमकिन उज्र कैसे चाँदी उनमें\n",
      "[0, 563, 251, 326, 237, 67, 529, 494, 536, 11, 1]\n",
      "हाथों पालियाँ हीला धन भाँति हवसपरस्त जायगा हिंदुस्तानी है\n",
      "[0, 367, 4, 1]\n",
      "[0, 507, 589, 274, 37, 559, 253, 86, 529, 33, 0, 287, 390, 299, 402, 316, 492, 130, 118, 162]\n",
      "लेने गुलामी नाम कहीं बेहद सारा बाजार हवसपरस्त वह SENTENCE_START दिये दिलों सुन व्यथित शब्द समझे सम्मुख देखना\n",
      "[0, 294, 402, 356, 280, 75, 461, 280, 100, 504, 59, 312, 236, 590, 406, 391, 215, 98, 422, 586]\n",
      "वस्त्रभूषणों व्यथित महिला चीजों कभी ढीठ चीजों सुख बोला अब रँगकर प्रान्तों करते माँगों आहें केंद्र उन जगमगाती\n",
      "[0, 569, 97, 576, 505, 423, 254, 206, 408, 183, 337, 90, 256, 519, 489, 101, 179, 580, 495, 329]\n",
      "निकली समय इसकी ऐ कांति नगर कठोरता पिरोती गलियों हतबुद्धि आग पहुँचा लोगों धड़क समस्त दुनिया इसका चाहे\n",
      "[0, 287, 393, 224, 344, 221, 214, 303, 405, 349, 207, 327, 295, 340, 56, 189, 156, 97, 444, 20]\n",
      "दिये जटित बनाव पहल भरे दिनों गये गूँथती शाप पैशाचिकता सुनेगा सजाकर मातम खुदा द्वार तब समय आवाजें\n",
      "[0, 236, 459, 333, 293, 564, 275, 215, 555, 469, 135, 522, 512, 569, 319, 313, 304, 163, 248, 154]\n",
      "प्रान्तों उनमें शिरोधार्य सुंदर गरदन लेती केंद्र कसम स्वरूप दी इसलिए गुमान निकली कूदना तेरा जिन ज्यादा होते\n",
      "[0, 52, 497, 387, 108, 360, 304, 523, 279, 234, 43, 567, 328, 541, 260, 105, 542, 305, 128, 458]\n",
      "दिया जाय आँखों उसे इनमें जिन करना पसंद ले हुक्म तैमूरी इतनी मलाल घर उसकी पैरों सूर्य कहना\n",
      "[0, 429, 49, 495, 174, 376, 574, 190, 533, 427, 223, 338, 589, 138, 98, 88, 271, 10, 100, 73]\n",
      "उड़ाती देखकर चाहे होता सोचने रह खैर ताने लजाती स्त्रियों सी गुलामी रक्षा उन लोग उठती को सुख\n",
      "[0, 558, 237, 458, 14, 23, 4, 15, 365, 19, 146, 2, 366, 90, 367, 25, 135, 16, 1]\n",
      "रखते धन रहीं हो रहा था पर इंद्रियलिप्सा ने जौहर की पुरानी आग ठंडी कर दी थी\n",
      "[0, 68, 403, 133, 28, 26, 1]\n",
      "[0, 185, 197, 459, 371, 274, 313, 176, 13, 349, 17, 9, 418, 357, 358, 57, 54, 144, 2, 463]\n",
      "बह निकाली उनमें सम्मान नाम तेरा मुमकिन एक शाप भी न गुजरने जिसकी निगाह कटार या तलवार की\n",
      "[0, 172, 292, 296, 264, 249, 204, 336, 407, 394, 591, 455, 574, 578, 415, 382, 54, 334, 143, 354]\n",
      "बाकी उनको लाओ चसका बटेरों हृदय सुनी मोतियाँ आभूषण रुखसत नीचा रह दिन उल्लंघन होकर या प्राण ऐसी\n",
      "[0, 6, 584, 141, 35, 296, 1]\n",
      "[0, 242, 437, 274, 199, 520, 285, 476, 545, 89, 30, 24, 60, 173, 13, 7, 547, 52, 1]\n",
      "बहाया क्षण नाम ईरानी अभी हथियार देव देतीं घरों गयीं तो मैंने तुम्हें एक और मौका दिया\n",
      "[0, 224, 146, 489, 231, 492, 85, 35, 276, 90, 187, 143, 579, 573, 271, 344, 550, 305, 272, 2]\n",
      "बनाव जौहर धड़क स्थान समझे तरफ रही संध्या आग हाहाकार ऐसी गिने जिंदा उठती पहल बंदी सूर्य उधर\n",
      "[0, 18, 502, 110, 3, 504, 24, 104, 8, 257, 49, 105, 50, 106, 30, 1]\n",
      "नादिरशाह कठोर सरदारों में बोला तो वहाँ का सामान देखकर उसकी आँखें खुल गयीं\n",
      "[0, 325, 340, 1]\n",
      "[0, 493, 287, 375, 160, 480, 97, 128, 579, 38, 316, 503, 241, 45, 428, 297, 38, 256, 426, 5]\n",
      "नाचा दिये मर्यादा सहसा नीचे समय कहना गिने किया शब्द शब्दों पानी जान लपटें खबरदार किया पहुँचा कलियों\n",
      "[0, 258, 519, 265, 283, 160, 423, 238, 591, 315, 243, 353, 587, 108, 38, 261, 264, 211, 193, 464]\n",
      "जन्म लोगों लगा बैठ सहसा कांति खिंच रुखसत शांत जाता लोचनों बचाओ उसे किया जीवन चसका धारण लगी\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model, senten_max_length)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6a4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a80de9c",
   "metadata": {},
   "source": [
    "# Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42fb0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2305c428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68bc6ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.4\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af60a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ebd00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_numbers(text):\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1f4bf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Heal the text\n",
    "text = ''\n",
    "print( \"Reading txt file...\")\n",
    "with open('./pariksha.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# sentence delimiter processing\n",
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_  \")\n",
    "text = text.replace(\":\", \" _comma_  \")\n",
    "text = text.replace(\";\", \" _comma_  \")\n",
    "\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "\n",
    "text = text.replace('\"',\"\")\n",
    "\n",
    "# i leave apostrophes in place, spawning separate words\n",
    "#text = text.replace(\"’\",\"\")\n",
    "\n",
    "# absorb tabs\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "text = text.replace(\"  \", \"\")\n",
    "\n",
    "# remove numbes\n",
    "text = clean_numbers(text)\n",
    "\n",
    "# absorb soace\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "text = _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0db1c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5741"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57a432fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है गलियों में खून की नदियाँ बह रही हैं चारों तरफ हाहाकार मचा हुआ है बाजार बंद हैं दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं किसी की जान सलामत नहीं है कहीं घरों में आग लगी हुई हैकहीं बाजार लुट रहा हैकोई किसी की फरियाद नहीं सुनता रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती मानव हृदय की क्रूरताकठोरता और पैशाचिकता अपना विकरालतम रूप धारण किये हुए हैं इसी समय नादिरशाह ने बादशाही महल में प्रवेश किया दिल्ली उन दिनों भोगविलास की केंद्र बनी हुई थी सजावट और तकल्लुफ के सामानों से रईसों के भवन भरे रहते थे स्त्रियों को बनाव सिंगार के सिवा कोई काम न था पुरुषों को सुख भोग के सिवा और कोई चिंता न थी राजनीति का स्थान शेरो शायरी ने ले लिया था समस्त प्रान्तों से धन खिंच खिंचकर दिल्ली आता था और पानी की भाँति बहाया जाता था वेश्याओं की चाँदी थी कहीं तीतरों के जोड़ होते थेकहीं बटेरों और बुलबुलों की पालियाँ ठनती थीं सारा नगर विलास निद्रा में मग्न था नादिरशाह शाही महल में पहुँचा तो वहाँ का सामान देखकर उसकी आँखें खुल गयीं उसका जन्म दरिद्र घर में हुआ था उसका समस्त जीवन रणभूमि में ही कटा था भोगविलास का उसे चसका न लगा था कहाँ रणक्षेत्र के कष्ट और कहाँ यह सुख साम्राज्य जिधर आँख उठती थीउधर से हटने का नाम न लेती थी संध्या हो गयी थी नादिरशाह अपने सरदारों के साथ महल की सैर करता और अपनी पसंद की चीजों को बटोरता हुआ दीवाने खास में आकर कारचोबी मसनद पर बैठ गयासरदारों को वहाँ से चले जाने का हुक्म दे दियाअपने सब हथियार खोलकर रख दिये और महल में द़रोगा को बुलाकर हुक्म दियामैं शाही बेगमों का नाच देखना चाहता हूँ तुम इसी वक्त उनको सुंदर वस्त्रभूषणों से सजाकर मेरे सामने लाओ खबरदारजरा भी देर न हो मैं कोई उज्र या इनकार नहीं सुन सकता दारोगा ने यह नादिरशाही हुक्म सुना तो होश उड़ गये वह महिलाएँ जिन पर कभी सूर्य की दृष्टि भी नहीं पड़ी कैसे इस मजलिस में आयेंगी नाचने का तो कहना ही क्या शाही बेगमों का इतना अपमान कभी न हुआ था हा नरपिशाच दिल्ली को खून से रँगकर भी तेरा चित्त शांत नहीं हुआ मगर नादिरशाह के सम्मुख एक शब्द भी जबान से निकालना अग्नि के मुख में कूदना था सिर झुकाकर आदाब बजा लाया और'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8d425",
   "metadata": {},
   "source": [
    "Create array of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f0b3160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है गलियों में खून की नदियाँ बह रही हैं चारों तरफ हाहाकार मचा हुआ है बाजार बंद हैं दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं किसी की जान सलामत नहीं है कहीं घरों में आग लगी हुई हैकहीं बाजार लुट रहा हैकोई किसी की फरियाद नहीं सुनता रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती मानव हृदय की क्रूरताकठोरता और पैशाचिकता अपना विकरालतम रूप धारण किये हुए हैं इसी समय नादिरशाह ने बादशाही महल में प्रवेश किया दिल्ली उन दिनों भोगविलास की केंद्र बनी हुई थी सजावट और तकल्लुफ के सामानों से रईसों के भवन भरे रहते थे स्त्रियों को बनाव सिंगार के सिवा कोई काम न था पुरुषों को सुख भोग के सिवा और कोई चिंता न थी राजनीति का स्थान शेरो शायरी ने ले लिया था समस्त प्रान्तों से धन खिंच खिंचकर दिल्ली आता था और पानी की भाँति बहाया जाता था वेश्याओं की चाँदी थी कहीं तीतरों के जोड़ होते थेकहीं बटेरों और बुलबुलों की पालियाँ ठनती थीं सारा नगर विलास निद्रा में मग्न था नादिरशाह शाही महल में पहुँचा तो वहाँ का सामान देखकर उसकी आँखें खुल गयीं उसका जन्म दरिद्र घर में हुआ था उसका समस्त जीवन रणभूमि में ही कटा था भोगविलास का उसे चसका न लगा था कहाँ रणक्षेत्र के कष्ट और कहाँ यह सुख साम्राज्य जिधर आँख उठती थीउधर से हटने का नाम न लेती थी संध्या हो गयी थी नादिरशाह अपने सरदारों के साथ महल की सैर करता और अपनी पसंद की चीजों को बटोरता हुआ दीवाने खास में आकर कारचोबी मसनद पर बैठ गयासरदारों को वहाँ से चले जाने का हुक्म दे दियाअपने सब हथियार खोलकर रख दिये और महल में द़रोगा को बुलाकर हुक्म दियामैं शाही बेगमों का नाच देखना चाहता हूँ तुम इसी वक्त उनको सुंदर वस्त्रभूषणों से सजाकर मेरे सामने लाओ खबरदारजरा भी देर न हो मैं कोई उज्र या इनकार नहीं सुन सकता दारोगा ने यह नादिरशाही हुक्म सुना तो होश उड़ गये वह महिलाएँ जिन पर कभी सूर्य की दृष्टि भी नहीं पड़ी कैसे इस मजलिस में आयेंगी नाचने का तो कहना ही क्या शाही बेगमों का इतना अपमान कभी न हुआ था हा नरपिशाच दिल्ली को खून से रँगकर भी तेरा चित्त शांत नहीं हुआ मगर नादिरशाह के सम्मुख एक शब्द भी जबान से निकालना अग्नि के मुख में कूदना था सिर झुकाकर आदाब बजा लाया और आकर रनिवास में सब बेगमों को नादिरशाही हुक्म सुना दियाउसके साथ ही यह इत्तला भी दे दी कि जरा भी ताम्मुल न होनादिरशाह कोई उज्र या हीला न सुनेगा शाही खानदान पर इतनी बड़ी विपत्ति कभी नहीं पड़ीपर इस समय विजयी बादशाह की आज्ञा को शिरोधार्य करने के सिवा प्राण रक्षा का अन्य कोई उपाय नहीं था बेगमों ने यह आज्ञा सुनी तो हतबुद्धि सी हो गयीं सारे रनिवास में मातम सा छा गया वह चहल पहल गायब हो गयी सैकड़ों हृदयों से इस अत्याचारी के प्रति एक शाप निकल गया किसी ने आकाश की ओर सहायतायाचक लोचनों से देखाकिसी ने खुदा और रसूल को सुमिरन कियापर ऐसी एक महिला भी न थी जिसकी निगाह कटार या तलवार की तरफ गयी हो यद्यपि इनमें कितनी ही बेगमों के नसों में राजपूतानियों का रक्त प्रवाहित हो रहा थापर इंद्रियलिप्सा ने जौहर की पुरानी आग ठंडी कर दी थी सुखभोग की लालसा आत्म सम्मान का सर्वनाश कर देती है आपस में सलाह करके मर्यादा की रक्षा का कोई उपाय सोचने की मुहलत न थी एक एक पल भाग्य का निर्णय कर रहा था हताश होकर सभी ललनाओं ने पापी के सम्मुख जाने का निश्चय किया आँखों से आँसू जारी थेदिलों से आहें निकल रही थींपर रत्न जटित आभूषण पहने जा रहे थेअश्रु सिंचित नेत्रों में सुरमा लगाया जा रहा था और शोक व्यथित हृदयों पर सुगंध का लेप किया जा रहा था कोई केश गूँथती थीकोई माँगों में मोतियाँ पिरोती थी एक भी ऐसे पक्के इरादे की स्त्री न थीजो ईश्वर पर अथवा अपनी टेक परइस आज्ञा का उल्लंघन करने का साहस कर सके एक घंटा भी न गुजरने पाया था कि बेगमात पूरे के पूरे आभूषणों से जगमगातीअपने मुख की कांति से बेले और गुलाब की कलियों को लजातीसुगंध की लपटें उड़ातीछमछम करती हुई दीवाने खास में आकर नादिरशाह के सामने खड़ी हो गयीं नादिरशाह ने एक बार कनखियों से परियों के इस दल को देखा और तब मसनद की टेक लगाकर लेट गया अपनी तलवार और कटार सामने रख दीं एक क्षण में उसकी आँखें झपकने लगीं उसने एक अँगड़ाई ली और करवट बदल ली जरा देर में उसके खर्राटों की आवाजें सुनायी देने लगीं ऐसा जान पड़ा कि वह गहरी निद्रा में मग्न हो गया है आध घंटे तक वह पड़ा सोता रहा और बेगमें ज्यों की त्यों सिर नीचा किये दीवार के चित्रों की भाँति खड़ी रहीं उनमें दो एक महिलाएँ जो ढीठ थींघूँघट की ओट से नादिरशाह को देख भी रही थीं और आपस में दबी जबान में कानाफूसी कर रही थींकैसा भयंकर स्वरूप है कितनी रणोन्मत्त आँखें हैं कितना भारी शरीर है आदमी काहे को हैदेव है सहसा नादिरशाह की आँखें खुल गयीं परियों का दल पूर्ववत् खड़ा था उसे जागते देखकर बेगमों ने सिर नीचे कर लिये और अंग समेटकर भेंड़ों की भाँति एक दूसरे से मिल गयीं सबके दिल धड़क रहे थे कि अब यह जालिम नाचने गाने को कहेगातब कैसे क्या होगा खुदा इस जालिम से समझे मगर नाचा तो न जायगा चाहे जान ही क्यों न जाय इससे ज्यादा जिल्लत अब न सही जायगी सहसा नादिरशाह कठोर शब्दों में बोलाऐ खुदा की बंदियोमैंने तुम्हारा इम्तहान लेने के लिए बुलाया था और अफसोस के साथ कहना पड़ता है कि तुम्हारी निसबत मेरा जो गुमान थावह हर्फ ब हर्फ सच निकला जब किसी कौम की औरतों में ग़ैरत नहीं रहती तो वह कौम मुरदा हो जाती है देखना चाहता था कि तुम लोगों में अभी कुछ ग़ैरत बाकी है या नहीं इसलिए मैंने तुम्हें यहाँ बुलाया था मैं तुम्हारी बेहुरमती नहीं करना चाहता था मैं इतना ऐश का बंदा नहीं हूँवरना आज भेड़ों के गल्ले चराता होता न इतना हवसपरस्त हूँवरना आज फारस में सरोद और सितार की ताने सुनता होताजिसका मजा मैं हिंदुस्तानी गाने से कहीं ज्यादा उठा सकता हूँ मुझे सिर्फ तुम्हारा इम्तहान लेना था मुझे यह देखकर सच्चा मलाल हो रहा है कि तुममें ग़ैरत का जौहर बाकी न रहा क्या यह मुमकिन न था कि तुम मेरे हुक्म को पैरों तले कुचल देतीं जब तुम यहाँ आ गयीं तो मैंने तुम्हें एक और मौका दिया मैंने नींद का बहाना किया क्या यह मुमकिन न था कि तुममें से कोई खुदा की बंदी इस कटार को उठाकर मेरे जिगर में चुभा देती मैं कलामेपाक की कसम खाकर कहता हूँ कि तुम में से किसी को कटार पर हाथ रखते देखकर मुझे बेहद खुशी होतीमैं उन नाजुक हाथों के सामने गरदन झुका देता पर अफसोस है कि आज तैमूरी खानदान की एक बेटी भी यहाँ ऐसी नहीं निकली जो अपनी हुरमत बिगाड़ने पर हाथ उठाती अब यह सल्तनत जिंदा नहीं रह सकती इसकी हस्ती के दिन गिने हुए हैं इसका निशान बहुत जल्द दुनिया से मिट जाएगा तुम लोग जाओ और हो सके तो अब भी सल्तनत को बचाओ वरना इसी तरह हवस की गुलामी करते हुए दुनिया से रुखसत हो जाओगी']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_data = text.split('.')\n",
    "# training_data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62041a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['नादिरशाह की सेना ने दिल्ली में कत्लेआम कर रखा है गलियों में खून की नदियाँ बह रही हैं चारों तरफ हाहाकार मचा हुआ है बाजार बंद हैं दिल्ली के लोग घरों के द्वार बंद किये जान की खैर मना रहे हैं किसी की जान सलामत नहीं है कहीं घरों में आग लगी हुई हैकहीं बाजार लुट रहा हैकोई किसी की फरियाद नहीं सुनता रईसों की बेगमें महलों से निकाली जा रही हैं और उनकी बेहुरमती की जाती है ईरानी सिपाहियों की रक्तपिपासा किसी तरह नहीं बुझती मानव हृदय की क्रूरताकठोरता और पैशाचिकता अपना विकरालतम रूप धारण किये हुए हैं इसी समय नादिरशाह ने बादशाही महल में प्रवेश किया दिल्ली उन दिनों भोगविलास की केंद्र बनी हुई थी सजावट और तकल्लुफ के सामानों से रईसों के भवन भरे रहते थे स्त्रियों को बनाव सिंगार के सिवा कोई काम न था पुरुषों को सुख भोग के सिवा और कोई चिंता न थी राजनीति का स्थान शेरो शायरी ने ले लिया था समस्त प्रान्तों से धन खिंच खिंचकर दिल्ली आता था और पानी की भाँति बहाया जाता था वेश्याओं की चाँदी थी कहीं तीतरों के जोड़ होते थेकहीं बटेरों और बुलबुलों की पालियाँ ठनती थीं सारा नगर विलास निद्रा में मग्न था नादिरशाह शाही महल में पहुँचा तो वहाँ का सामान देखकर उसकी आँखें खुल गयीं उसका जन्म दरिद्र घर में हुआ था उसका समस्त जीवन रणभूमि में ही कटा था भोगविलास का उसे चसका न लगा था कहाँ रणक्षेत्र के कष्ट और कहाँ यह सुख साम्राज्य जिधर आँख उठती थीउधर से हटने का नाम न लेती थी संध्या हो गयी थी नादिरशाह अपने सरदारों के साथ महल की सैर करता और अपनी पसंद की चीजों को बटोरता हुआ दीवाने खास में आकर कारचोबी मसनद पर बैठ गयासरदारों को वहाँ से चले जाने का हुक्म दे दियाअपने सब हथियार खोलकर रख दिये और महल में द़रोगा को बुलाकर हुक्म दियामैं शाही बेगमों का नाच देखना चाहता हूँ तुम इसी वक्त उनको सुंदर वस्त्रभूषणों से सजाकर मेरे सामने लाओ खबरदारजरा भी देर न हो मैं कोई उज्र या इनकार नहीं सुन सकता दारोगा ने यह नादिरशाही हुक्म सुना तो होश उड़ गये वह महिलाएँ जिन पर कभी सूर्य की दृष्टि भी नहीं पड़ी कैसे इस मजलिस में आयेंगी नाचने का तो कहना ही क्या शाही बेगमों का इतना अपमान कभी न हुआ था हा नरपिशाच दिल्ली को खून से रँगकर भी तेरा चित्त शांत नहीं हुआ मगर नादिरशाह के सम्मुख एक शब्द भी जबान से निकालना अग्नि के मुख में कूदना था सिर झुकाकर आदाब बजा लाया और आकर रनिवास में सब बेगमों को नादिरशाही हुक्म सुना दियाउसके साथ ही यह इत्तला भी दे दी कि जरा भी ताम्मुल न होनादिरशाह कोई उज्र या हीला न सुनेगा शाही खानदान पर इतनी बड़ी विपत्ति कभी नहीं पड़ीपर इस समय विजयी बादशाह की आज्ञा को शिरोधार्य करने के सिवा प्राण रक्षा का अन्य कोई उपाय नहीं था बेगमों ने यह आज्ञा सुनी तो हतबुद्धि सी हो गयीं सारे रनिवास में मातम सा छा गया वह चहल पहल गायब हो गयी सैकड़ों हृदयों से इस अत्याचारी के प्रति एक शाप निकल गया किसी ने आकाश की ओर सहायतायाचक लोचनों से देखाकिसी ने खुदा और रसूल को सुमिरन कियापर ऐसी एक महिला भी न थी जिसकी निगाह कटार या तलवार की तरफ गयी हो यद्यपि इनमें कितनी ही बेगमों के नसों में राजपूतानियों का रक्त प्रवाहित हो रहा थापर इंद्रियलिप्सा ने जौहर की पुरानी आग ठंडी कर दी थी सुखभोग की लालसा आत्म सम्मान का सर्वनाश कर देती है आपस में सलाह करके मर्यादा की रक्षा का कोई उपाय सोचने की मुहलत न थी एक एक पल भाग्य का निर्णय कर रहा था हताश होकर सभी ललनाओं ने पापी के सम्मुख जाने का निश्चय किया आँखों से आँसू जारी थेदिलों से आहें निकल रही थींपर रत्न जटित आभूषण पहने जा रहे थेअश्रु सिंचित नेत्रों में सुरमा लगाया जा रहा था और शोक व्यथित हृदयों पर सुगंध का लेप किया जा रहा था कोई केश गूँथती थीकोई माँगों में मोतियाँ पिरोती थी एक भी ऐसे पक्के इरादे की स्त्री न थीजो ईश्वर पर अथवा अपनी टेक परइस आज्ञा का उल्लंघन करने का साहस कर सके एक घंटा भी न गुजरने पाया था कि बेगमात पूरे के पूरे आभूषणों से जगमगातीअपने मुख की कांति से बेले और गुलाब की कलियों को लजातीसुगंध की लपटें उड़ातीछमछम करती हुई दीवाने खास में आकर नादिरशाह के सामने खड़ी हो गयीं नादिरशाह ने एक बार कनखियों से परियों के इस दल को देखा और तब मसनद की टेक लगाकर लेट गया अपनी तलवार और कटार सामने रख दीं एक क्षण में उसकी आँखें झपकने लगीं उसने एक अँगड़ाई ली और करवट बदल ली जरा देर में उसके खर्राटों की आवाजें सुनायी देने लगीं ऐसा जान पड़ा कि वह गहरी निद्रा में मग्न हो गया है आध घंटे तक वह पड़ा सोता रहा और बेगमें ज्यों की त्यों सिर नीचा किये दीवार के चित्रों की भाँति खड़ी रहीं उनमें दो एक महिलाएँ जो ढीठ थींघूँघट की ओट से नादिरशाह को देख भी रही थीं और आपस में दबी जबान में कानाफूसी कर रही थींकैसा भयंकर स्वरूप है कितनी रणोन्मत्त आँखें हैं कितना भारी शरीर है आदमी काहे को हैदेव है सहसा नादिरशाह की आँखें खुल गयीं परियों का दल पूर्ववत् खड़ा था उसे जागते देखकर बेगमों ने सिर नीचे कर लिये और अंग समेटकर भेंड़ों की भाँति एक दूसरे से मिल गयीं सबके दिल धड़क रहे थे कि अब यह जालिम नाचने गाने को कहेगातब कैसे क्या होगा खुदा इस जालिम से समझे मगर नाचा तो न जायगा चाहे जान ही क्यों न जाय इससे ज्यादा जिल्लत अब न सही जायगी सहसा नादिरशाह कठोर शब्दों में बोलाऐ खुदा की बंदियोमैंने तुम्हारा इम्तहान लेने के लिए बुलाया था और अफसोस के साथ कहना पड़ता है कि तुम्हारी निसबत मेरा जो गुमान थावह हर्फ ब हर्फ सच निकला जब किसी कौम की औरतों में ग़ैरत नहीं रहती तो वह कौम मुरदा हो जाती है देखना चाहता था कि तुम लोगों में अभी कुछ ग़ैरत बाकी है या नहीं इसलिए मैंने तुम्हें यहाँ बुलाया था मैं तुम्हारी बेहुरमती नहीं करना चाहता था मैं इतना ऐश का बंदा नहीं हूँवरना आज भेड़ों के गल्ले चराता होता न इतना हवसपरस्त हूँवरना आज फारस में सरोद और सितार की ताने सुनता होताजिसका मजा मैं हिंदुस्तानी गाने से कहीं ज्यादा उठा सकता हूँ मुझे सिर्फ तुम्हारा इम्तहान लेना था मुझे यह देखकर सच्चा मलाल हो रहा है कि तुममें ग़ैरत का जौहर बाकी न रहा क्या यह मुमकिन न था कि तुम मेरे हुक्म को पैरों तले कुचल देतीं जब तुम यहाँ आ गयीं तो मैंने तुम्हें एक और मौका दिया मैंने नींद का बहाना किया क्या यह मुमकिन न था कि तुममें से कोई खुदा की बंदी इस कटार को उठाकर मेरे जिगर में चुभा देती मैं कलामेपाक की कसम खाकर कहता हूँ कि तुम में से किसी को कटार पर हाथ रखते देखकर मुझे बेहद खुशी होतीमैं उन नाजुक हाथों के सामने गरदन झुका देता पर अफसोस है कि आज तैमूरी खानदान की एक बेटी भी यहाँ ऐसी नहीं निकली जो अपनी हुरमत बिगाड़ने पर हाथ उठाती अब यह सल्तनत जिंदा नहीं रह सकती इसकी हस्ती के दिन गिने हुए हैं इसका निशान बहुत जल्द दुनिया से मिट जाएगा तुम लोग जाओ और हो सके तो अब भी सल्तनत को बचाओ वरना इसी तरह हवस की गुलामी करते हुए दुनिया से रुखसत हो जाओगी']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(training_data)):\n",
    "    training_data[i] = training_data[i].strip()\n",
    "training_data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ee4d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Set the maximum sequence length for padding\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(training_data)\n",
    " \n",
    "# Convert the training data into sequences of tokens\n",
    "sequences = tokenizer.texts_to_sequences(training_data)\n",
    " \n",
    "# Pad the sequences to have the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f193fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 1, 170, 15, 31, 2, 171, 24, 172, 10, 173, 2, 77, 1, 174, 175, 32, 25, 176, 78, 177, 178, 33, 10, 79, 80, 25, 31, 3, 81, 82, 3, 179, 80, 53, 38, 1, 180, 181, 54, 25, 26, 1, 38, 182, 11, 10, 55, 82, 2, 83, 183, 56, 184, 79, 185, 20, 186, 26, 1, 187, 11, 84, 85, 1, 86, 188, 4, 189, 39, 32, 25, 5, 190, 87, 1, 88, 10, 191, 192, 1, 193, 26, 89, 11, 194, 195, 196, 1, 197, 5, 198, 199, 200, 201, 202, 53, 57, 25, 58, 90, 17, 15, 203, 40, 2, 204, 41, 31, 91, 205, 92, 1, 206, 207, 56, 18, 208, 5, 209, 3, 210, 4, 85, 3, 211, 212, 213, 93, 214, 9, 215, 216, 3, 59, 21, 217, 8, 6, 218, 9, 94, 219, 3, 59, 5, 21, 220, 8, 18, 221, 7, 222, 223, 224, 15, 225, 226, 6, 95, 227, 4, 228, 229, 230, 31, 231, 6, 5, 232, 1, 60, 233, 234, 6, 235, 1, 236, 18, 55, 237, 3, 238, 239, 240, 241, 5, 242, 1, 243, 244, 96, 245, 246, 247, 97, 2, 98, 6, 17, 42, 40, 2, 248, 22, 99, 7, 249, 43, 100, 44, 101, 27, 102, 250, 251, 252, 2, 33, 6, 102, 95, 253, 254, 2, 34, 255, 6, 92, 7, 103, 256, 8, 257, 6, 104, 258, 3, 259, 5, 104, 19, 94, 260, 261, 262, 263, 264, 4, 265, 7, 266, 8, 267, 18, 268, 13, 61, 18, 17, 269, 270, 3, 62, 40, 1, 271, 272, 5, 45, 273, 1, 274, 9, 275, 33, 105, 106, 2, 63, 276, 107, 23, 277, 278, 9, 99, 4, 279, 108, 7, 35, 109, 280, 110, 281, 282, 111, 283, 5, 40, 2, 284, 9, 285, 35, 286, 42, 28, 7, 287, 112, 64, 65, 29, 58, 288, 289, 290, 291, 4, 292, 66, 46, 293, 294, 14, 113, 8, 13, 36, 21, 114, 47, 295, 11, 296, 115, 297, 15, 19, 116, 35, 117, 22, 298, 299, 300, 37, 118, 301, 23, 67, 302, 1, 303, 14, 11, 304, 119, 30, 305, 2, 306, 120, 7, 22, 121, 34, 48, 42, 28, 7, 68, 307, 67, 8, 33, 6, 308, 309, 31, 9, 77, 4, 310, 14, 311, 312, 313, 11, 33, 122, 17, 3, 123, 12, 314, 14, 124, 4, 315, 316, 3, 125, 2, 317, 6, 69, 318, 319, 320, 321, 5, 63, 126, 2, 110, 28, 9, 116, 35, 117, 322, 62, 34, 19, 323, 14, 109, 127, 16, 128, 14, 324, 8, 325, 21, 114, 47, 326, 8, 327, 42, 129, 23, 328, 329, 330, 67, 11, 331, 30, 90, 332, 333, 1, 70, 9, 334, 130, 3, 59, 335, 131, 7, 336, 21, 132, 11, 6, 28, 15, 19, 70, 337, 22, 338, 339, 13, 27, 340, 126, 2, 341, 342, 343, 49, 37, 344, 345, 346, 13, 61, 347, 133, 4, 30, 348, 3, 349, 12, 350, 134, 49, 26, 15, 351, 1, 352, 353, 354, 4, 355, 15, 50, 5, 356, 9, 357, 358, 135, 12, 359, 14, 8, 18, 360, 361, 51, 47, 136, 1, 78, 61, 13, 362, 363, 137, 34, 28, 3, 364, 2, 365, 7, 366, 367, 13, 20, 368, 369, 15, 138, 1, 370, 83, 371, 24, 127, 18, 372, 1, 373, 374, 375, 7, 376, 24, 139, 10, 140, 2, 377, 378, 379, 1, 131, 7, 21, 132, 380, 1, 381, 8, 18, 12, 12, 382, 383, 7, 384, 24, 20, 6, 385, 386, 387, 388, 15, 389, 3, 123, 108, 7, 390, 41, 391, 4, 392, 393, 394, 4, 395, 134, 32, 396, 397, 398, 399, 400, 39, 54, 401, 402, 403, 2, 404, 405, 39, 20, 6, 5, 406, 407, 133, 23, 408, 7, 409, 41, 39, 20, 6, 21, 410, 411, 412, 413, 2, 414, 415, 18, 12, 14, 416, 417, 418, 1, 419, 8, 420, 421, 23, 422, 45, 141, 423, 70, 7, 424, 130, 7, 425, 24, 142, 12, 426, 14, 8, 427, 428, 6, 16, 429, 143, 3, 143, 430, 4, 431, 125, 1, 432, 4, 433, 5, 434, 1, 435, 9, 436, 1, 437, 438, 439, 56, 105, 106, 2, 63, 17, 3, 46, 144, 13, 27, 17, 15, 12, 440, 441, 4, 145, 3, 30, 146, 9, 442, 5, 443, 107, 1, 141, 444, 445, 49, 45, 136, 5, 51, 46, 111, 446, 12, 447, 2, 100, 44, 448, 147, 449, 12, 450, 148, 5, 451, 452, 148, 128, 113, 2, 453, 454, 1, 455, 456, 457, 147, 458, 38, 149, 16, 37, 459, 97, 2, 98, 13, 49, 10, 460, 461, 462, 37, 149, 463, 20, 5, 86, 464, 1, 465, 69, 466, 53, 467, 3, 468, 1, 60, 144, 469, 470, 471, 12, 118, 71, 472, 473, 1, 474, 4, 17, 9, 475, 14, 32, 96, 5, 140, 2, 476, 124, 2, 477, 24, 32, 478, 479, 480, 10, 137, 481, 44, 25, 482, 483, 484, 10, 485, 486, 9, 487, 10, 150, 17, 1, 44, 101, 27, 145, 7, 146, 488, 489, 6, 103, 490, 43, 28, 15, 69, 491, 24, 492, 5, 493, 494, 495, 1, 60, 12, 496, 4, 497, 27, 498, 499, 500, 54, 93, 16, 52, 19, 151, 120, 152, 9, 501, 119, 48, 502, 50, 30, 151, 4, 503, 122, 504, 22, 8, 505, 506, 38, 34, 507, 8, 508, 509, 153, 510, 52, 8, 511, 512, 150, 17, 513, 514, 2, 515, 50, 1, 516, 154, 155, 517, 3, 518, 156, 6, 5, 157, 3, 62, 121, 519, 10, 16, 158, 520, 521, 71, 522, 523, 159, 524, 159, 525, 526, 160, 26, 161, 1, 527, 2, 72, 11, 528, 22, 37, 161, 529, 13, 88, 10, 112, 64, 6, 16, 29, 530, 2, 531, 532, 72, 162, 10, 47, 11, 533, 73, 163, 74, 156, 6, 36, 158, 87, 11, 534, 64, 6, 36, 68, 535, 7, 536, 11, 164, 75, 537, 3, 538, 539, 540, 8, 68, 541, 164, 75, 542, 2, 543, 5, 544, 1, 545, 84, 546, 547, 36, 548, 152, 4, 55, 153, 549, 115, 65, 76, 550, 154, 155, 551, 6, 76, 19, 43, 552, 553, 13, 20, 10, 16, 165, 72, 7, 138, 162, 8, 20, 48, 19, 166, 8, 6, 16, 29, 66, 35, 9, 554, 555, 556, 557, 160, 29, 74, 558, 27, 22, 73, 163, 12, 5, 559, 560, 73, 561, 7, 562, 41, 48, 19, 166, 8, 6, 16, 165, 4, 21, 50, 1, 563, 30, 51, 9, 564, 66, 565, 2, 566, 139, 36, 567, 1, 568, 569, 570, 65, 16, 29, 2, 4, 26, 9, 51, 23, 167, 571, 43, 76, 572, 573, 574, 91, 575, 576, 3, 46, 577, 578, 579, 23, 157, 10, 16, 75, 580, 129, 1, 12, 581, 14, 74, 135, 11, 582, 71, 45, 583, 584, 23, 167, 585, 52, 19, 168, 586, 11, 587, 588, 589, 590, 3, 591, 592, 57, 25, 593, 594, 595, 596, 169, 4, 597, 598, 29, 81, 599, 5, 13, 142, 22, 52, 14, 168, 9, 600, 601, 58, 89, 602, 1, 603, 604, 57, 169, 4, 605, 13, 606]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e4b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139  36 567   1 568 569 570  65  16  29   2   4  26   9  51  23 167 571\n",
      "  43  76 572 573 574  91 575 576   3  46 577 578 579  23 157  10  16  75\n",
      " 580 129   1  12 581  14  74 135  11 582  71  45 583 584  23 167 585  52\n",
      "  19 168 586  11 587 588 589 590   3 591 592  57  25 593 594 595 596 169\n",
      "   4 597 598  29  81 599   5  13 142  22  52  14 168   9 600 601  58  89\n",
      " 602   1 603 604  57 169   4 605  13 606]\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e45f1d",
   "metadata": {},
   "source": [
    "# Naive training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "376f587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input and output data for training\n",
    "X = padded_sequences[:, :-1]\n",
    "y = padded_sequences[:, -1]\n",
    "\n",
    "# Get the total number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e256827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139  36 567   1 568 569 570  65  16  29   2   4  26   9  51  23 167 571\n",
      "  43  76 572 573 574  91 575 576   3  46 577 578 579  23 157  10  16  75\n",
      " 580 129   1  12 581  14  74 135  11 582  71  45 583 584  23 167 585  52\n",
      "  19 168 586  11 587 588 589 590   3 591 592  57  25 593 594 595 596 169\n",
      "   4 597 598  29  81 599   5  13 142  22  52  14 168   9 600 601  58  89\n",
      " 602   1 603 604  57 169   4 605  13 606] 100\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0], len(padded_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2634518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139  36 567   1 568 569 570  65  16  29   2   4  26   9  51  23 167 571\n",
      "  43  76 572 573 574  91 575 576   3  46 577 578 579  23 157  10  16  75\n",
      " 580 129   1  12 581  14  74 135  11 582  71  45 583 584  23 167 585  52\n",
      "  19 168 586  11 587 588 589 590   3 591 592  57  25 593 594 595 596 169\n",
      "   4 597 598  29  81 599   5  13 142  22  52  14 168   9 600 601  58  89\n",
      " 602   1 603 604  57 169   4 605  13] 99\n"
     ]
    }
   ],
   "source": [
    "print(X[0], len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89776483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571787aa",
   "metadata": {},
   "source": [
    "# Complete training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a576eb6",
   "metadata": {},
   "source": [
    "Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c72537f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "#for s in sequences:\n",
    "print(sequences[0])\n",
    "for s in [sequences[0]]:\n",
    "    #seq_array = s.split()\n",
    "    words_in_s = [s[0]]\n",
    "    for w in s[1:]:\n",
    "        X.append([v for v in words_in_s])\n",
    "        y.append(w)\n",
    "        words_in_s.append(w)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55011a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/sanikadhayabar/anaconda3/lib/python3.11/site-packages (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "106aaff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 62883.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "junk = [str(i) for i in range(10)]\n",
    "from tqdm import tqdm\n",
    "for s in tqdm(junk):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea4c4e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 67.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1203, 1203)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for s in tqdm(sequences):\n",
    "    if 0 < len(s):\n",
    "        words_in_s = [s[0]]\n",
    "        for w in s[1:]:\n",
    "            X.append([v for v in words_in_s])\n",
    "            y.append(w)\n",
    "            words_in_s.append(w)\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "001680c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 1, 170, 15, 31, 2, 171, 24, 172, 10, 173, 2, 77, 1, 174, 175, 32, 25, 176, 78, 177, 178, 33, 10, 79, 80, 25, 31, 3, 81, 82, 3, 179, 80, 53, 38, 1, 180, 181, 54, 25, 26, 1, 38, 182, 11, 10, 55, 82, 2, 83, 183, 56, 184, 79, 185, 20, 186, 26, 1, 187, 11, 84, 85, 1, 86, 188, 4, 189, 39, 32, 25, 5, 190, 87, 1, 88, 10, 191, 192, 1, 193, 26, 89, 11, 194, 195, 196, 1, 197, 5, 198, 199, 200, 201, 202, 53, 57, 25, 58, 90] 17\n"
     ]
    }
   ],
   "source": [
    "print(X[100], y[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84ca9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 1, 170, 15, 31, 2, 171, 24, 172, 10, 173, 2, 77, 1, 174, 175, 32, 25, 176, 78, 177, 178, 33, 10, 79, 80, 25, 31, 3, 81, 82, 3, 179, 80, 53, 38, 1, 180, 181, 54, 25, 26, 1, 38, 182, 11, 10, 55, 82, 2, 83, 183, 56, 184, 79, 185, 20, 186, 26, 1, 187, 11, 84, 85, 1, 86, 188, 4, 189, 39, 32, 25, 5, 190, 87, 1, 88, 10, 191, 192, 1, 193, 26, 89, 11, 194, 195, 196, 1, 197, 5, 198, 199, 200, 201, 202, 53, 57, 25, 58, 90, 17] 15\n"
     ]
    }
   ],
   "source": [
    "print(X[101], y[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "457a2d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "padded_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eb86f04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 170, 15, 31, 2, 171, 24, 172, 10, 173]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42de0bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = np.array(y)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71059f04",
   "metadata": {},
   "source": [
    "# Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "575803b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fasttext\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mdownload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# English\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ft \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc.en.300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fasttext/FastText.py:445\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path):\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _FastText(model_path\u001b[38;5;241m=\u001b[39mpath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fasttext/FastText.py:93\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mfasttext()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mloadModel(model_path)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a01f273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d3c07a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext.util.reduce_model(ft, 100)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9233036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हिन्दी विकि डीवीडी परियोजना -0.078365 0.049841 -0.23186 0.072641 -0.10667 0.15986 -0.036957 -0.039915 -0.17246 0.070029 0.16518 0.092211 -0.1706 -0.11882 0.11917 0.0092635 -0.20663 0.041592 -0.10816 0.06173 0.13963 0.23366 -0.060042 0.30493 0.14802 0.029197 -0.10574 0.034834 0.18872 0.038719 -0.036717 0.21376 -0.0070195 -0.14029 0.071023 -0.18044 0.016243 -0.095747 -0.097626 -0.068943 0.059256 -0.12054 -0.065286 -0.24157 0.036331 -0.14777 -0.22512 0.05606 0.0035369 -0.2494 0.047777 0.024651 -0.013128 -0.23639 0.038466 0.062365 -0.11298 -0.067375 0.099474 0.011086 -0.062023 -0.060376 0.10178 -0.0066213 0.17612 -0.014129 -0.053058 0.24642 -0.0038288 -0.30613 -0.24535 -0.23448 -0.079723 -0.010486 0.068892 0.084526 0.06854 -0.13237 0.10715 -0.14325 -0.17184 0.20093 0.16292 0.14244 0.21243 0.03807 -0.21012 -0.31086 -0.021485 0.12033 0.060714 -0.027038 0.20331 0.030021 -0.39067 -0.0098092 0.15615 0.0041575 0.17791 -0.081739 0.11193 -0.053256 0.10047 0.011498 -0.088826 0.20969 -0.040423 0.24449 0.066236 -0.082111 0.036556 -0.039248 -0.02011 -0.12246 -0.069376 0.032821 0.12677 0.039534 -0.092619 0.1807 0.22061 -0.039044 -0.22346 -0.13551 -0.065507 0.038692 0.14319 0.092603 -0.045684 -0.0047913 -0.0079866 0.0060002 -0.1402 -0.069391 -0.0019816 0.004622 -0.22528 -0.26915 -0.0096326 0.39776 -0.11617 0.13252 -0.13893 0.18866 -0.075462 0.080481 0.088816 -0.1501 -0.23335 -0.0088571 -0.029746 -0.084251 0.046373 -0.019399 0.11205 -0.14402 0.0032386 0.20223 -0.22877 -0.24094 0.10437 -0.10931 0.26135 -0.13482 -0.16121 -0.037952 0.17203 -0.13757 -0.0013978 0.30631 -0.05499 0.066195 -0.15927 0.268 0.095519 0.093642 0.083128 -0.18578 0.14099 -0.13505 -0.21431 0.26425 -0.17616 0.17065 -0.073169 0.20672 0.26876 -0.25248 -0.076466 -0.02241 0.16161 -0.1787 0.04891 -0.074224 0.33418 -0.10822 0.27853 0.011056 -0.23107 0.052632 0.043238 0.093672 -0.013565 -0.0043547 -0.13003 -0.25392 -0.38329 0.11827 -0.28815 0.028203 -0.22723 0.07816 0.23039 -0.012378 0.056551 -0.1395 -0.0028789 0.25031 0.15897 0.018888 -0.13251 0.1725 0.10357 -0.18051 -0.046143 -0.17101 -0.1258 0.083772 0.10192 0.06628 0.022771 0.20813 -0.033328 -0.2406 -0.051467 -0.00095692 0.25995 -0.054842 0.013706 0.00024795 0.061175 0.06304 -0.042725 -0.10456 0.20379 -0.14827 0.23111 0.17068 0.18913 -0.10744 -0.17335 -0.02538 -0.070823 0.034547 -0.10387 0.041082 -0.084267 -0.050602 0.060069 -0.017042 0.22878 0.0042483 -0.0798 -0.10257 -0.1143 0.38138 0.24482 0.11387 -0.08282 -0.13242 -0.0042191 0.29918 0.20509 -0.23409 -0.13579 0.011493 0.22479 0.11025 0.09081 0.10672 0.19877 -0.035146 -0.042073 0.00075463 -0.0042505 0.036672 -0.088108 0.0091228 0.030616 -0.23643 -0.069865 -0.11101 0.022933 -0.10471 0.067176 -0.23911 -0.21566 -0.10519 -0.34334 -0.099426 \n",
      "\n",
      "Found 50797 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as  np\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "indicft = \"./wiki.hi/wiki.hi.vec\"\n",
    "\n",
    "embeddings_index = {} #initialize dictionary\n",
    "f = open(indicft, encoding='utf8')\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # print(values)\n",
    "        word = values[0]\n",
    "        # print(word)\n",
    "        if isinstance(values[1],float):\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        else:\n",
    "            coefs = np.asarray(values[2:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "except:\n",
    "    print(line)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d88f1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8fc6c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 700)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdf7119",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_matrix[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0a7e5",
   "metadata": {},
   "source": [
    "# Build model architecture\n",
    "## Single LSTM layer\n",
    "With same number of neurons as the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87108e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d16940b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60700"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d784621",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db152b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60700"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39d77ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 50, input_length=self.max_sequence_length-1))\n",
    "model.add(Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06158ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 700)         424900    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               320400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 607)               61307     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 806607 (3.08 MB)\n",
      "Trainable params: 381707 (1.46 MB)\n",
      "Non-trainable params: 424900 (1.62 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c5f27",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc726b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 3s 71ms/step - loss: 6.3140 - accuracy: 0.0283\n",
      "CPU times: user 4.79 s, sys: 1.96 s, total: 6.75 s\n",
      "Wall time: 3.25 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16be1dd90>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(padded_X, y2, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71791022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.875"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.75 * 150 / 60 #if 6.75 in min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54216bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 6.0345 - accuracy: 0.0266\n",
      "Epoch 2/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.9382 - accuracy: 0.0341\n",
      "Epoch 3/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.9141 - accuracy: 0.0341\n",
      "Epoch 4/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.9065 - accuracy: 0.0341\n",
      "Epoch 5/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8987 - accuracy: 0.0341\n",
      "Epoch 6/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.8955 - accuracy: 0.0341\n",
      "Epoch 7/150\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 5.8963 - accuracy: 0.0341\n",
      "Epoch 8/150\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 5.8900 - accuracy: 0.0341\n",
      "Epoch 9/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8925 - accuracy: 0.0341\n",
      "Epoch 10/150\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 5.8910 - accuracy: 0.0341\n",
      "Epoch 11/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8885 - accuracy: 0.0341\n",
      "Epoch 12/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8882 - accuracy: 0.0341\n",
      "Epoch 13/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8866 - accuracy: 0.0341\n",
      "Epoch 14/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8880 - accuracy: 0.0341\n",
      "Epoch 15/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8896 - accuracy: 0.0341\n",
      "Epoch 16/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8835 - accuracy: 0.0341\n",
      "Epoch 17/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8852 - accuracy: 0.0341\n",
      "Epoch 18/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.8853 - accuracy: 0.0341\n",
      "Epoch 19/150\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 5.8844 - accuracy: 0.0341\n",
      "Epoch 20/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8838 - accuracy: 0.0341\n",
      "Epoch 21/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8826 - accuracy: 0.0341\n",
      "Epoch 22/150\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 5.8822 - accuracy: 0.0341\n",
      "Epoch 23/150\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 5.8812 - accuracy: 0.0341\n",
      "Epoch 24/150\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 5.8822 - accuracy: 0.0341\n",
      "Epoch 25/150\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 5.8806 - accuracy: 0.0341\n",
      "Epoch 26/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8810 - accuracy: 0.0341\n",
      "Epoch 27/150\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 5.8825 - accuracy: 0.0341\n",
      "Epoch 28/150\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 5.8794 - accuracy: 0.0341\n",
      "Epoch 29/150\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 5.8804 - accuracy: 0.0341\n",
      "Epoch 30/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8804 - accuracy: 0.0341\n",
      "Epoch 31/150\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 5.8788 - accuracy: 0.0341\n",
      "Epoch 32/150\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 5.8795 - accuracy: 0.0341\n",
      "Epoch 33/150\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 5.8807 - accuracy: 0.0341\n",
      "Epoch 34/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8793 - accuracy: 0.0341\n",
      "Epoch 35/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8799 - accuracy: 0.0341\n",
      "Epoch 36/150\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 5.8776 - accuracy: 0.0341\n",
      "Epoch 37/150\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 5.8784 - accuracy: 0.0341\n",
      "Epoch 38/150\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 5.8781 - accuracy: 0.0341\n",
      "Epoch 39/150\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 5.8791 - accuracy: 0.0341\n",
      "Epoch 40/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8773 - accuracy: 0.0341\n",
      "Epoch 41/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8777 - accuracy: 0.0341\n",
      "Epoch 42/150\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 5.8776 - accuracy: 0.0341\n",
      "Epoch 43/150\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 5.8777 - accuracy: 0.0341\n",
      "Epoch 44/150\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 5.8761 - accuracy: 0.0341\n",
      "Epoch 45/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8774 - accuracy: 0.0341\n",
      "Epoch 46/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8761 - accuracy: 0.0341\n",
      "Epoch 47/150\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 5.8762 - accuracy: 0.0341\n",
      "Epoch 48/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8756 - accuracy: 0.0341\n",
      "Epoch 49/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8745 - accuracy: 0.0341\n",
      "Epoch 50/150\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 5.8759 - accuracy: 0.0341\n",
      "Epoch 51/150\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 5.8762 - accuracy: 0.0341\n",
      "Epoch 52/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8759 - accuracy: 0.0341\n",
      "Epoch 53/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8750 - accuracy: 0.0341\n",
      "Epoch 54/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8739 - accuracy: 0.0341\n",
      "Epoch 55/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8756 - accuracy: 0.0341\n",
      "Epoch 56/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8751 - accuracy: 0.0341\n",
      "Epoch 57/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8750 - accuracy: 0.0341\n",
      "Epoch 58/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8749 - accuracy: 0.0341\n",
      "Epoch 59/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8746 - accuracy: 0.0341\n",
      "Epoch 60/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8749 - accuracy: 0.0341\n",
      "Epoch 61/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8734 - accuracy: 0.0341\n",
      "Epoch 62/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8740 - accuracy: 0.0341\n",
      "Epoch 63/150\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 5.8739 - accuracy: 0.0341\n",
      "Epoch 64/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8727 - accuracy: 0.0341\n",
      "Epoch 65/150\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.8733 - accuracy: 0.0341\n",
      "Epoch 66/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8742 - accuracy: 0.0341\n",
      "Epoch 67/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8732 - accuracy: 0.0341\n",
      "Epoch 68/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8731 - accuracy: 0.0341\n",
      "Epoch 69/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8733 - accuracy: 0.0341\n",
      "Epoch 70/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8734 - accuracy: 0.0341\n",
      "Epoch 71/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8716 - accuracy: 0.0341\n",
      "Epoch 72/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8738 - accuracy: 0.0341\n",
      "Epoch 73/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8728 - accuracy: 0.0341\n",
      "Epoch 74/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8728 - accuracy: 0.0341\n",
      "Epoch 75/150\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 5.8739 - accuracy: 0.0341\n",
      "Epoch 76/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8727 - accuracy: 0.0341\n",
      "Epoch 77/150\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 5.8737 - accuracy: 0.0341\n",
      "Epoch 78/150\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 5.8722 - accuracy: 0.0341\n",
      "Epoch 79/150\n",
      "38/38 [==============================] - 4s 93ms/step - loss: 5.8721 - accuracy: 0.0341\n",
      "Epoch 80/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8712 - accuracy: 0.0341\n",
      "Epoch 81/150\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 5.8719 - accuracy: 0.0341\n",
      "Epoch 82/150\n",
      "38/38 [==============================] - 3s 89ms/step - loss: 5.8719 - accuracy: 0.0341\n",
      "Epoch 83/150\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 5.8714 - accuracy: 0.0341\n",
      "Epoch 84/150\n",
      "38/38 [==============================] - 4s 93ms/step - loss: 5.8716 - accuracy: 0.0341\n",
      "Epoch 85/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8715 - accuracy: 0.0341\n",
      "Epoch 86/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8713 - accuracy: 0.0341\n",
      "Epoch 87/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8718 - accuracy: 0.0341\n",
      "Epoch 88/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8720 - accuracy: 0.0341\n",
      "Epoch 89/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8706 - accuracy: 0.0341\n",
      "Epoch 90/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8720 - accuracy: 0.0341\n",
      "Epoch 91/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8710 - accuracy: 0.0341\n",
      "Epoch 92/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 93/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 94/150\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 5.8711 - accuracy: 0.0341\n",
      "Epoch 95/150\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 5.8710 - accuracy: 0.0341\n",
      "Epoch 96/150\n",
      "38/38 [==============================] - 3s 84ms/step - loss: 5.8711 - accuracy: 0.0341\n",
      "Epoch 97/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 98/150\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 5.8703 - accuracy: 0.0341\n",
      "Epoch 99/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8709 - accuracy: 0.0341\n",
      "Epoch 100/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8713 - accuracy: 0.0341\n",
      "Epoch 101/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 102/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8704 - accuracy: 0.0341\n",
      "Epoch 103/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8698 - accuracy: 0.0341\n",
      "Epoch 104/150\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 105/150\n",
      "38/38 [==============================] - 4s 94ms/step - loss: 5.8696 - accuracy: 0.0341\n",
      "Epoch 106/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8697 - accuracy: 0.0341\n",
      "Epoch 107/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8693 - accuracy: 0.0341\n",
      "Epoch 108/150\n",
      "38/38 [==============================] - 3s 81ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 109/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8713 - accuracy: 0.0341\n",
      "Epoch 110/150\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 5.8686 - accuracy: 0.0341\n",
      "Epoch 111/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8699 - accuracy: 0.0341\n",
      "Epoch 112/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8693 - accuracy: 0.0341\n",
      "Epoch 113/150\n",
      "38/38 [==============================] - 3s 83ms/step - loss: 5.8705 - accuracy: 0.0341\n",
      "Epoch 114/150\n",
      "38/38 [==============================] - 3s 86ms/step - loss: 5.8691 - accuracy: 0.0341\n",
      "Epoch 115/150\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 5.8694 - accuracy: 0.0341\n",
      "Epoch 116/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8697 - accuracy: 0.0341\n",
      "Epoch 117/150\n",
      "38/38 [==============================] - 3s 85ms/step - loss: 5.8693 - accuracy: 0.0341\n",
      "Epoch 118/150\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 5.8696 - accuracy: 0.0341\n",
      "Epoch 119/150\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 5.8698 - accuracy: 0.0341\n",
      "Epoch 120/150\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 5.8697 - accuracy: 0.0341\n",
      "Epoch 121/150\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 5.8692 - accuracy: 0.0341\n",
      "Epoch 122/150\n",
      "38/38 [==============================] - 4s 95ms/step - loss: 5.8691 - accuracy: 0.0341\n",
      "Epoch 123/150\n",
      "38/38 [==============================] - 4s 100ms/step - loss: 5.8692 - accuracy: 0.0341\n",
      "Epoch 124/150\n",
      "38/38 [==============================] - 3s 82ms/step - loss: 5.8689 - accuracy: 0.0341\n",
      "Epoch 125/150\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 5.8697 - accuracy: 0.0341\n",
      "Epoch 126/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.8682 - accuracy: 0.0341\n",
      "Epoch 127/150\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 5.8688 - accuracy: 0.0341\n",
      "Epoch 128/150\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 5.8697 - accuracy: 0.0341\n",
      "Epoch 129/150\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 5.8695 - accuracy: 0.0341\n",
      "Epoch 130/150\n",
      "38/38 [==============================] - 3s 66ms/step - loss: 5.8688 - accuracy: 0.0341\n",
      "Epoch 131/150\n",
      "38/38 [==============================] - 2s 64ms/step - loss: 5.8680 - accuracy: 0.0341\n",
      "Epoch 132/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8686 - accuracy: 0.0341\n",
      "Epoch 133/150\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 5.8690 - accuracy: 0.0341\n",
      "Epoch 134/150\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 5.8683 - accuracy: 0.0341\n",
      "Epoch 135/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8685 - accuracy: 0.0341\n",
      "Epoch 136/150\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 5.8679 - accuracy: 0.0341\n",
      "Epoch 137/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.8682 - accuracy: 0.0341\n",
      "Epoch 138/150\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 5.8678 - accuracy: 0.0341\n",
      "Epoch 139/150\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 5.8679 - accuracy: 0.0341\n",
      "Epoch 140/150\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 5.8679 - accuracy: 0.0341\n",
      "Epoch 141/150\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 5.8679 - accuracy: 0.0341\n",
      "Epoch 142/150\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 5.8686 - accuracy: 0.0341\n",
      "Epoch 143/150\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 5.8695 - accuracy: 0.0341\n",
      "Epoch 144/150\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 5.8687 - accuracy: 0.0341\n",
      "Epoch 145/150\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 5.8675 - accuracy: 0.0341\n",
      "Epoch 146/150\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 5.8676 - accuracy: 0.0341\n",
      "Epoch 147/150\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 5.8678 - accuracy: 0.0341\n",
      "Epoch 148/150\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 5.8674 - accuracy: 0.0341\n",
      "Epoch 149/150\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 5.8666 - accuracy: 0.0341\n",
      "Epoch 150/150\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 5.8691 - accuracy: 0.0341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a08f5cd0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_X, y2, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499e844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
